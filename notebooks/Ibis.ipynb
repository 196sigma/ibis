{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibis\n",
    "Created: 09/13/2024\\\n",
    "Updated: 09/25/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import add_trading_hours\n",
    "from fredapi import Fred\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST SET COMPUTING ENVIRONMENT\n",
    "COMPUTING_ENV = 'windows'\n",
    "#COMPUTING_ENV = 'ubuntu'\n",
    "#COMPUTING_ENV = 'aws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTING_ENV == 'windows':\n",
    "    WORKING_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\ibis\"\n",
    "    API_KEYS_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\API_KEYS\"\n",
    "elif COMPUTING_ENV == 'ubuntu':\n",
    "    WORKING_DIR = \"/home/reggie//Dropbox/ibis\"\n",
    "    API_KEYS_DIR = \"/home/reggie/Dropbox/API_KEYS\"\n",
    "elif COMPUTING_ENV == 'aws':\n",
    "    WORKING_DIR = \"/home/ubuntu/ibis\"\n",
    "    API_KEYS_DIR = \"/home/ubuntu/API_KEYS\"\n",
    "\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "STOCK_DATA_DIR  = os.path.join(DATA_DIR, 'tmp')\n",
    "FRD_DATA_DIR = os.path.join(DATA_DIR, 'frd-historical')\n",
    "print(f\"Working directory is\\n\\t{WORKING_DIR}\")\n",
    "print(f\"Data directory is\\n\\t{DATA_DIR}\")\n",
    "print(f\"Stock data directory is\\n\\t{STOCK_DATA_DIR}\")\n",
    "print(f\"FRD data directory is\\n\\t{FRD_DATA_DIR}\")\n",
    "\n",
    "# data dictionary\n",
    "data_dictionary_fp = os.path.join(DATA_DIR, \"data_dictionary.json\")\n",
    "if not os.path.exists(data_dictionary_fp):\n",
    "    print(f\"Data dictionary does not exist in {data_dictionary_fp}. Initializing empty data dictionary.\")\n",
    "    data_dictionary = {}\n",
    "else:\n",
    "    print(f\"Data dictionary exists in {data_dictionary_fp}. Loading data dictionary.\")\n",
    "    with open(data_dictionary_fp, \"r\") as f:\n",
    "        data_dictionary = json.load(f)\n",
    "    print('\\tKeys:', data_dictionary.keys())\n",
    "\n",
    "# OpenAI API key\n",
    "openai_api_key_fp = os.path.join(API_KEYS_DIR, 'openai-api-key-1.txt')\n",
    "with open(openai_api_key_fp) as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "print(f\"OpenAI API key is loaded\")\n",
    "\n",
    "# FRED Data\n",
    "fred_api_key_fp = os.path.join(API_KEYS_DIR, 'FRED-API-KEY')\n",
    "with open(fred_api_key_fp) as f:\n",
    "    fred_api_key = f.read().strip()\n",
    "print(f\"FRED API key loaded\")\n",
    "FRED_DIR = os.path.join(DATA_DIR, \"FRED\")\n",
    "fred_daily_data_fp = os.path.join(FRED_DIR, 'daily', 'FRED_daily.csv')\n",
    "if not os.path.exists(fred_daily_data_fp):\n",
    "    print(f\"FRED daily data do not exist in {fred_daily_data_fp}.\")\n",
    "else:\n",
    "    print(f\"FRED daily data are in {fred_daily_data_fp}\")\n",
    "\n",
    "# a table to map asset types to download directories\n",
    "frd_download_directories = pd.read_csv(os.path.join(FRD_DATA_DIR, 'frd-download-directories.csv'))\n",
    "frd_download_directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_prices(ticker, asset_type, period='full', timeframe='1min', adjustment='adj_splitdiv', data_dir='./'):\n",
    "    if asset_type == 'stock':\n",
    "        csv_filename = f\"{ticker}_{period}_{timeframe}_{adjustment.replace('_','')}.txt\"\n",
    "        fp = os.path.join(data_dir, csv_filename)\n",
    "        prices_df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\",\",\n",
    "            names=['date', 'open', 'high', 'low', 'close', 'volume'],\n",
    "            header=0,  # Assuming the first row is a header, if not set to None\n",
    "            on_bad_lines='warn',  # Skip bad lines\n",
    "            engine='python'  # Use the Python engine for more flexible error handling\n",
    "        )\n",
    "    elif asset_type == 'index':\n",
    "        csv_filename = f\"{ticker}_{period}_{timeframe}.txt\"\n",
    "        fp = os.path.join(data_dir, csv_filename)\n",
    "        prices_df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\",\",\n",
    "            names=['date', 'open', 'high', 'low', 'close',],\n",
    "            header=0,  # Assuming the first row is a header, if not set to None\n",
    "            on_bad_lines='warn',  # Skip bad lines\n",
    "            engine='python'  # Use the Python engine for more flexible error handling\n",
    "        )\n",
    "\n",
    "    # Convert 'date' column to datetime if it's not already\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    \n",
    "    # Extract the day as YYYY-MM-DD\n",
    "    prices_df['day'] = prices_df['date'].dt.date\n",
    "    \n",
    "    if period in ['1min', '5min', '30min', '1hour']:\n",
    "        # Extract the time as HH:MM:SS\n",
    "        prices_df['time'] = prices_df['date'].dt.time\n",
    "        \n",
    "        # Calculate the time ID (minute of the day from 1 to 1440)\n",
    "        prices_df['time_id'] = prices_df['date'].dt.hour * 60 + prices_df['date'].dt.minute + 1\n",
    "    \n",
    "        prices_df.set_index('date', inplace=True)\n",
    "        prices_df = add_trading_hours(prices_df)\n",
    "    else:\n",
    "        prices_df.set_index('date', inplace=True)\n",
    "\n",
    "    # add returns\n",
    "    prices_df.sort_index(inplace=True, ascending=True)\n",
    "    prices_df['open_to_close_ret'] = prices_df['close']/prices_df['open'] - 1\n",
    "    prices_df['close_to_close_ret'] = prices_df['close'].pct_change()\n",
    "    prices_df['overnight_ret'] = prices_df['open']/prices_df['close'].shift(1) - 1\n",
    "    prices_df['open_to_high_ret'] = prices_df['high']/prices_df['open'] - 1\n",
    "    prices_df['open_to_low_ret'] = prices_df['low']/prices_df['open'] - 1\n",
    "    prices_df['low_to_high'] = prices_df['high']/prices_df['low'] - 1 # max possible return\n",
    "\n",
    "    prices_df['ticker'] = ticker\n",
    "    \n",
    "    return prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_type = 'index'\n",
    "ticker = 'SPX'\n",
    "period = 'full'\n",
    "timeframe = '1day'\n",
    "index_data_dir = os.path.join(FRD_DATA_DIR, frd_download_directories.query(f\"type == '{asset_type}' & timeframe == '{timeframe}'\")['directory'].values[0], 'csv')\n",
    "print(f\"Index data directory is {index_data_dir}\")\n",
    "spx_df = load_stock_prices(\n",
    "    ticker=ticker,\n",
    "    asset_type=asset_type,\n",
    "    period=period,\n",
    "    timeframe=timeframe,\n",
    "    data_dir=index_data_dir\n",
    ")\n",
    "spx_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_from_gpt_markdown(content: str, save: bool = False, filename: str = None):\n",
    "    \"\"\"This function takes in a ChatGPT response that is in Markdown form with Python code blocks and returns the Python code as a string.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        content : str : The ChatGPT response in Markdown form\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        python_str : str : The Python code as a string\n",
    "    \"\"\"\n",
    "    python_str = \"\"\n",
    "    in_python_block = False\n",
    "    for line in content.split(\"\\n\"):\n",
    "        if line.startswith(\"```python\"):\n",
    "            in_python_block = True\n",
    "        elif line.startswith(\"```\"):\n",
    "            in_python_block = False\n",
    "        elif in_python_block:\n",
    "            python_str += line + \"\\n\"\n",
    "    if save:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(python_str)\n",
    "    return python_str\n",
    "\n",
    "def gpt_code(system_prompt: str, user_prompt: str, filename, save, model='gpt-4-o') -> (str, str):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return content, parse_python_from_gpt_markdown(content, save=False, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downoad FRED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key=fred_api_key)\n",
    "\n",
    "fred_tips_series = ['DFII10', 'DFII5', 'DFII20', 'DFII30',]\n",
    "fred_treasury_series = ['DGS10', 'DGS2', 'DGS30']\n",
    "\n",
    "fred_tips_daily_data_fp = os.path.join(FRED_DIR, 'daily', 'FRED_daily_tips.csv')\n",
    "fred_treasuries_daily_data_fp = os.path.join(FRED_DIR, 'daily', 'FRED_daily_treasuries.csv')\n",
    "\n",
    "create_or_update_fred_tips_data = False\n",
    "create_or_update_fred_treasuries_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treasury Inflation-Indexed Securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if create_or_update_fred_tips_data:\n",
    "\n",
    "    # create or update data dictionary\n",
    "    for series in fred_tips_series:\n",
    "        if series not in data_dictionary:\n",
    "            data_dictionary[series] = dict(fred.get_series_info(series))\n",
    "        else:\n",
    "            print(f\"{series} already in data dictionary.\")\n",
    "\n",
    "    # export data_dictionary to json\n",
    "    with open(data_dictionary_fp, 'w') as f:\n",
    "        json.dump(data_dictionary, f, indent=2)\n",
    "\n",
    "    tips_df = pd.DataFrame({\n",
    "        series: fred.get_series(series) for series in fred_tips_series\n",
    "    })\n",
    "    tips_df.reset_index(inplace=True)\n",
    "    tips_df.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "    tips_df['date'] = pd.to_datetime(tips_df['date'])\n",
    "    tips_df.set_index('date', inplace=True)\n",
    "    tips_df.to_csv(fred_tips_daily_data_fp)\n",
    "else:\n",
    "    tips_df = pd.read_csv(fred_tips_daily_data_fp, index_col='date', parse_dates=True)\n",
    "tips_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treasury Constant Maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_or_update_fred_treasuries_data:\n",
    "    for series in fred_treasury_series:\n",
    "        if series not in data_dictionary:\n",
    "            data_dictionary[series] = dict(fred.get_series_info(series))\n",
    "        else:\n",
    "            print(f\"{series} already in data dictionary.\")\n",
    "            \n",
    "    # export data_dictionary to json\n",
    "    with open(data_dictionary_fp, 'w') as f:\n",
    "        json.dump(data_dictionary, f, indent=2)\n",
    "\n",
    "    treasuries_df = pd.DataFrame({\n",
    "        series: fred.get_series(series) for series in fred_treasury_series\n",
    "    })\n",
    "    treasuries_df.reset_index(inplace=True)\n",
    "    treasuries_df.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "    treasuries_df['date'] = pd.to_datetime(treasuries_df['date'])\n",
    "    treasuries_df.set_index('date', inplace=True)\n",
    "    treasuries_df.to_csv(fred_treasuries_daily_data_fp)\n",
    "else:\n",
    "    treasuries_df = pd.read_csv(fred_treasuries_daily_data_fp, index_col='date', parse_dates=True)\n",
    "treasuries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Inflation and Interest Rate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Series and Plot Levels and Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_tips_daily_df = pd.read_csv(fred_tips_daily_data_fp, index_col='date', parse_dates=True)\n",
    "X = fred_tips_daily_df.copy()\n",
    "X.ffill(inplace=True)\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "# add pct change\n",
    "X_pct_change = X.pct_change()\n",
    "X_pct_change.dropna(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in X.columns:\n",
    "    plt.plot(X.index, X[col], label=col)\n",
    "plt.title(\"Treasury Inflation-Indexed Securities (TIPS)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in X_pct_change.columns:\n",
    "    plt.plot(X_pct_change.index, X_pct_change[col], label=col)\n",
    "plt.title(\"Treasury Inflation-Indexed Securities (TIPS) Percent Change\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_pct_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_pct_change.corr()\n",
    "print(corr.round(2))\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "print(corr.round(2))\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD on daily treasuries\n",
    "U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# s contains the singular values (variance explained by each component)\n",
    "# U contains the left singular vectors (temporal patterns)\n",
    "# Vt contains the right singular vectors (relationships between variables)\n",
    "\n",
    "# Retain only top k components for dimensionality reduction\n",
    "k = 3\n",
    "U_k = U[:, :k]\n",
    "S_k = np.diag(s[:k])\n",
    "Vt_k = Vt[:k, :]\n",
    "\n",
    "# Low-rank approximation of X\n",
    "X_approx = np.dot(U_k, np.dot(S_k, Vt_k))\n",
    "\n",
    "# Singular values show the importance of each component\n",
    "print(\"Top singular values:\", s[:10])\n",
    "\n",
    "# Use Vt_k to analyze relationships between variables (columns of X)\n",
    "print(\"Right singular vectors (V^T):\\n\", Vt_k)\n",
    "\n",
    "# first two SVs as factors\n",
    "factors = U_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot (Singular Values)\n",
    "A scree plot displays the magnitude of the singular values, which tells you how much variance each component explains. This can help identify how many components are important and where the diminishing returns occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 's' contains the singular values from SVD\n",
    "def plot_scree(s):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(s) + 1), s, marker='o', linestyle='-')\n",
    "    plt.title(\"Scree Plot of Singular Values\")\n",
    "    plt.xlabel(\"Component Number\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_scree(s[:50])  # Plot the first 50 singular values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Variance Explained\n",
    "\n",
    "Another useful plot is the cumulative explained variance, which shows how much total variance is explained as you include more singular values. It helps to decide how many components are necessary to capture most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_variance(s):\n",
    "    explained_variance = np.cumsum(s**2) / np.sum(s**2)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(s) + 1), explained_variance, marker='o', linestyle='-')\n",
    "    plt.title(\"Cumulative Explained Variance\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_cumulative_variance(s[:50])  # Plot cumulative variance for the first 50 components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Top Singular Vectors (Right Singular Vectors $V^T$)\n",
    "\n",
    "You can visualize the right singular vectors, which describe how the variables (columns of the original data) contribute to each principal component. A heatmap can highlight the relationship between variables and components.\n",
    "\n",
    "This heatmap shows the contribution of each time series (variables) to the top principal components. Patterns, correlations, and groups of similar variables will be visible, showing how variables are related to each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_singular_vectors(Vt, k=10):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(Vt[:k, :], cmap='coolwarm', center=0)\n",
    "    plt.title(f\"Heatmap of Top {k} Right Singular Vectors (V^T)\")\n",
    "    plt.xlabel(\"Variables (Time Series)\")\n",
    "    plt.ylabel(\"Singular Vector Index\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_singular_vectors(Vt_k, k=10)  # Plot top 10 right singular vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Projections (Left Singular Vectors U)\n",
    "\n",
    "The left singular vectors represent temporal patterns. You can project the original time series data onto the top components and visualize these projections to understand the key dynamics of the system over time.\n",
    "\n",
    "This plot shows how the time series evolve in terms of the most important components. You can spot trends, cycles, or other important temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_projections(U, components=[0, 1]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for comp in components:\n",
    "        plt.plot(U[:, comp], label=f\"Component {comp+1}\")\n",
    "    \n",
    "    plt.title(\"Time Series Projections onto Top Components\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Projection\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_time_series_projections(U_k, components=[0, 1])  # Plot the first two components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D/3D Scatter Plot of Time Series in Reduced Space\n",
    "\n",
    "By projecting the time series into a reduced space (using the top singular vectors), you can visualize the relationships between time steps or time series in lower dimensions (e.g., a 2D or 3D scatter plot).\n",
    "\n",
    "These projections show how the data points cluster in lower dimensions, helping to identify groups, trends, or anomalies. In 2D or 3D, clusters or separations between groups of time steps or variables may become more evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_projection(U, Vt, singular_values):\n",
    "    # Project the data onto the first two components\n",
    "    projection = np.dot(U[:, :2], np.diag(singular_values[:2]))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(projection[:, 0], projection[:, 1], alpha=0.5, s=10)\n",
    "    plt.title(\"2D Projection of Time Series Data\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_2d_projection(U_k, Vt_k, s[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_projection(U, singular_values):\n",
    "    # Project the data onto the first three components\n",
    "    projection = np.dot(U[:, :3], np.diag(singular_values[:3]))\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(projection[:, 0], projection[:, 1], projection[:, 2], alpha=0.5, s=10)\n",
    "    \n",
    "    ax.set_title(\"3D Projection of Time Series Data\")\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_3d_projection(U_k, s[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Rates II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_df.merge(treasuries_df, left_index=True, right_index=True, suffixes=('_tips', '_treasuries'))\n",
    "# rename 'date' index to 'time_index'\n",
    "X.index.rename('time_index', inplace=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of minutes in a year\n",
    "minutes_in_year = 525600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "n = 100_000 # number of minutes\n",
    "X = np.random.randn(n, k)\n",
    "time_index = pd.date_range(start='2020-01-01', periods=len(X), freq='T')  # 'T' for minute frequency\n",
    "X = pd.DataFrame(X, index=time_index).round(2)\n",
    "X.columns = [f\"feature_{i}\" for i in range(k)]\n",
    "print(X.shape)\n",
    "X.head()\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import cwt, ricker\n",
    "from scipy.fft import fft, fftfreq\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('./figures', exist_ok=True)\n",
    "os.makedirs('./tables', exist_ok=True)\n",
    "\n",
    "# Assume your dataframe is X\n",
    "# Replace 'target_column_name' with your actual target column name\n",
    "target_col = 'feature_0'\n",
    "target_series = X[target_col]\n",
    "\n",
    "# List of temporal resolutions to examine\n",
    "# Resample to coarser frequencies\n",
    "resolutions = ['5T', '15T', '30T', '1H', '1D']  # 5 Minutes, 15 Minutes, 30 Minutes, Hourly, Daily\n",
    "\n",
    "for res in resolutions:\n",
    "    print(f\"\\nAnalyzing data resampled to {res} resolution.\")\n",
    "    \n",
    "    # Resample the data\n",
    "    X_resampled = X.resample(res).mean()\n",
    "    target_resampled = X_resampled[target_col]\n",
    "    \n",
    "    # Save resampled target series\n",
    "    target_resampled.to_csv(f'./tables/target_series_{res}.csv')\n",
    "    \n",
    "    # 1. Correlation Analysis at the current resolution\n",
    "    correlations = X_resampled.corrwith(target_resampled).sort_values(ascending=False)\n",
    "    top_correlations = correlations.head(10)\n",
    "    print(f\"Top correlated series with the target series at {res} resolution:\")\n",
    "    print(top_correlations)\n",
    "    \n",
    "    # Save top correlations to a CSV file\n",
    "    top_correlations.to_csv(f'./tables/top_correlated_series_{res}.csv', header=['Correlation'])\n",
    "    \n",
    "    # Plot the top correlated series with the target series\n",
    "    top_correlated_series = correlations.index[1:6]  # Exclude the first one (itself)\n",
    "    for col in top_correlated_series:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(target_resampled.index, target_resampled.values, label='Target Series')\n",
    "        plt.plot(target_resampled.index, X_resampled[col].values, label=f'Series {col}')\n",
    "        plt.legend()\n",
    "        plt.title(f'Target Series vs Series {col} at {res} resolution')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        # Save the plot\n",
    "        plt.savefig(f'./figures/target_vs_series_{col}_{res}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Stationarity Tests (ADF Test)\n",
    "    adf_result = adfuller(target_resampled.dropna())\n",
    "    adf_output = pd.Series(adf_result[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    print(f\"ADF Test Result for Target Series at {res} resolution:\")\n",
    "    print(adf_output)\n",
    "    \n",
    "    # Save ADF test result\n",
    "    adf_output.to_csv(f'./tables/adf_test_{res}.csv', header=['Value'])\n",
    "    \n",
    "    # 3. Time Series Decomposition\n",
    "    # Since STL requires a frequency, we need to set it based on the resampled data\n",
    "    # For example, if resampling to '1H', the period might be 24 (hours in a day)\n",
    "    freq_dict = {'5T': 288, '15T': 96, '30T': 48, '1H': 24, '1D': 7}  # Adjusted periods\n",
    "    freq = freq_dict[res]\n",
    "    stl = STL(target_resampled.dropna(), period=freq)\n",
    "    stl_result = stl.fit()\n",
    "    \n",
    "    # Plot the decomposition\n",
    "    stl_result.plot()\n",
    "    plt.suptitle(f'STL Decomposition of Target Series at {res} resolution')\n",
    "    plt.savefig(f'./figures/stl_decomposition_{res}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save decomposition components\n",
    "    decomposition_df = pd.DataFrame({\n",
    "        'Trend': stl_result.trend,\n",
    "        'Seasonal': stl_result.seasonal,\n",
    "        'Residual': stl_result.resid\n",
    "    }, index=target_resampled.dropna().index)\n",
    "    decomposition_df.to_csv(f'./tables/stl_decomposition_{res}.csv')\n",
    "    \n",
    "    # 4. Spectral Analysis (Fourier Transform)\n",
    "    n = len(target_resampled.dropna())\n",
    "    yf = fft(target_resampled.dropna().values)  # Convert Series to NumPy array\n",
    "    xf = fftfreq(n, 1)[:n//2]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(xf, 2.0/n * np.abs(yf[0:n//2]))\n",
    "    plt.title(f'Frequency Domain of Target Series at {res} resolution')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.savefig(f'./figures/frequency_domain_{res}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save frequency and amplitude data\n",
    "    freq_ampl_df = pd.DataFrame({'Frequency': xf, 'Amplitude': 2.0/n * np.abs(yf[0:n//2])})\n",
    "    freq_ampl_df.to_csv(f'./tables/frequency_domain_{res}.csv', index=False)\n",
    "    \n",
    "    # 5. Wavelet Transform (Time-Frequency Analysis)\n",
    "    widths = np.arange(1, 31)\n",
    "    cwt_matr = cwt(target_resampled.fillna(0).values, ricker, widths)  # Convert Series to NumPy array\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(cwt_matr, extent=[0, len(target_resampled), 1, 31], cmap='PRGn', aspect='auto',\n",
    "               vmax=abs(cwt_matr).max(), vmin=-abs(cwt_matr).max())\n",
    "    plt.title(f'Continuous Wavelet Transform of Target Series at {res} resolution')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Scale')\n",
    "    plt.savefig(f'./figures/cwt_{res}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Note: Saving the entire wavelet matrix may not be practical due to size\n",
    "    \n",
    "    # 6. PCA at the current resolution\n",
    "    pca = PCA(n_components=5)\n",
    "    principal_components = pca.fit_transform(X_resampled.fillna(0))\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"Explained Variance Ratios by Principal Components at {res} resolution:\")\n",
    "    for i, ev in enumerate(explained_variance):\n",
    "        print(f\"PC{i+1}: {ev:.4f}\")\n",
    "    \n",
    "    # Save explained variance ratios\n",
    "    ev_df = pd.DataFrame({'Explained Variance Ratio': explained_variance},\n",
    "                         index=[f'PC{i+1}' for i in range(len(explained_variance))])\n",
    "    ev_df.to_csv(f'./tables/pca_explained_variance_{res}.csv')\n",
    "    \n",
    "    # Correlate PCs with target series\n",
    "    pc_df = pd.DataFrame(principal_components, index=X_resampled.index, columns=[f'PC{i+1}' for i in range(5)])\n",
    "    pc_df[target_col] = target_resampled.values\n",
    "    pc_correlations = pc_df.corr()[target_col][:-1]  # Exclude the target column itself\n",
    "    print(f\"Correlation between Target Series and Principal Components at {res} resolution:\")\n",
    "    print(pc_correlations)\n",
    "    \n",
    "    # Save PC correlations with target series\n",
    "    pc_correlations.to_csv(f'./tables/pc_correlations_with_target_{res}.csv', header=['Correlation'])\n",
    "    \n",
    "    # Plot the target series with principal components\n",
    "    for pc in pc_df.columns[:-1]:  # Exclude the target column\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(target_resampled.index, target_resampled.values, label='Target Series')\n",
    "        plt.plot(target_resampled.index, pc_df[pc].values, label=pc)\n",
    "        plt.legend()\n",
    "        plt.title(f'Target Series vs {pc} at {res} resolution')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        # Save the plot\n",
    "        plt.savefig(f'./figures/target_vs_{pc}_{res}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. Autocorrelation and Partial Autocorrelation Plots at the current resolution\n",
    "    # Determine the maximum number of lags based on data length\n",
    "    max_lags = min(20, len(target_resampled.dropna()) - 1)\n",
    "    \n",
    "    if max_lags > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plot_acf(target_resampled.dropna(), lags=max_lags)\n",
    "        plt.title(f'Autocorrelation Function of Target Series at {res} resolution')\n",
    "        plt.savefig(f'./figures/target_series_acf_{res}.png')\n",
    "        plt.close()\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plot_pacf(target_resampled.dropna(), lags=max_lags)\n",
    "        plt.title(f'Partial Autocorrelation Function of Target Series at {res} resolution')\n",
    "        plt.savefig(f'./figures/target_series_pacf_{res}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"Not enough data to compute autocorrelation at {res} resolution.\")\n",
    "    \n",
    "    # Optional: Granger Causality Tests at the current resolution\n",
    "    max_lag = 10\n",
    "    test_results = {}\n",
    "    for col in top_correlated_series:\n",
    "        data = X_resampled[[target_col, col]].dropna()\n",
    "        if len(data) > max_lag:\n",
    "            test = grangercausalitytests(data, maxlag=max_lag, verbose=False)\n",
    "            p_values = [round(test[i+1][0]['ssr_chi2test'][1], 4) for i in range(max_lag)]\n",
    "            test_results[col] = p_values\n",
    "        else:\n",
    "            print(f\"Not enough data for Granger Causality Test at {res} resolution for series {col}.\")\n",
    "            test_results[col] = [np.nan]*max_lag\n",
    "    \n",
    "    gc_df = pd.DataFrame(test_results, index=range(1, max_lag+1))\n",
    "    print(f\"Granger Causality Test P-values at {res} resolution:\")\n",
    "    print(gc_df)\n",
    "    \n",
    "    # Save Granger causality test results\n",
    "    gc_df.to_csv(f'./tables/granger_causality_pvalues_{res}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDFN\n",
    "\n",
    "* Log(Price) transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "ticker = 'RDFN'\n",
    "asset_type = \"stock\"        # Example values: stock, etf, futures, crypto, index, fx\n",
    "period = \"full\"             # Example values: full, month, week, day\n",
    "timeframe = \"1min\"          # Example values: 1min, 5min, 30min, 1hour, 1day\n",
    "adjustment = \"adj_splitdiv\"    # Example values: adj_split, adj_splitdiv, UNADJUSTED\n",
    "print(stock_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfn_1min_df = load_stock_prices(ticker, asset_type, period, timeframe, adjustment, STOCK_DATA_DIR)\n",
    "print(rdfn_1min_df.info())\n",
    "rdfn_1min_df[['open', 'high', 'low', 'close', 'volume']] = rdfn_1min_df[['open', 'high', 'low', 'close', 'volume']].apply(np.log)\n",
    "rdfn_1min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt\n",
    "stock_csv_filename = f\"{ticker}_{period}_{timeframe}_{adjustment.replace('_','')}.txt\"\n",
    "rdfn_df = pd.read_csv(os.path.join(DATA_DIR, stock_csv_filename.replace('.txt', '.csv')), parse_dates=True, index_col='date')\n",
    "rdfn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfn_1min_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot high\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rdfn_df['high'].values, label=f\"{ticker} High\")\n",
    "plt.title(f\"{ticker} High Price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# line plot volume\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rdfn_df['volume'].values, label=f\"{ticker} Volume\")\n",
    "plt.title(f\"{ticker} Volume\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily resampling\n",
    "rdfn_daily_df = rdfn_1min_df.resample('D').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "})\n",
    "rdfn_daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDFN and Treasuries and Interest Rates Exploration\n",
    "`09/25/2024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = pd.read_csv(os.path.join(FRED_DIR, 'daily', 'FRED_daily_tips.csv'), parse_dates=True, index_col='date')\n",
    "tips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treasuries_df = pd.read_csv(os.path.join(FRED_DIR, 'daily', 'FRED_daily_treasuries.csv'), parse_dates=True, index_col='date')\n",
    "treasuries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_df = tips_df.join(treasuries_df, how='inner')\n",
    "macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rdfn_daily_df.join(macro_df, how='left')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.query(\"volume > 0\").query(\"date >= '2024-09-01'\").query(\"date < '2024-09-13'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(X.index, X['DFII10'], label='DFII10')\n",
    "# title\n",
    "plt.title(\"Treasury Inflation-Indexed Securities (TIPS)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDFN and Directional Moves\n",
    "\n",
    "**Data**\n",
    "* ffill missing prices (should only be on non-trading days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfn_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rdfn_daily_df.copy()\n",
    "X.ffill(inplace=True)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write `X.info()` to string buffer\n",
    "import io\n",
    "\n",
    "with io.StringIO() as buffer:\n",
    "    X.info(buf=buffer)\n",
    "    info_str = buffer.getvalue()\n",
    "print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The columns of `X` are open, high, low, close, and volume; X has as `DatetimeIndex` with daily frequency named \\\"date.\\\"\"\"\"\n",
    "prompt += \"\\n\"\n",
    "prompt += \"\"\"The data go from {0} to {1}.\"\"\".format(X.index[0], X.index[-1])\n",
    "prompt += \"\\n\"\n",
    "prompt += \"\"\"The data have {0} rows and {1} columns.\"\"\".format(X.shape[0], X.shape[1]) \n",
    "prompt += \"\\n\"\n",
    "prompt += \"\"\"Here is the result of `X.describe()` and `X.info()`:\n",
    "\"\"\"\n",
    "\n",
    "prompt += \"\\n\"\n",
    "\n",
    "prompt += \"\"\"```\"\"\"\n",
    "prompt += '\\n'\n",
    "prompt += X.describe().to_string()\n",
    "prompt += '\\n'\n",
    "prompt += \"\"\"```\"\"\"\n",
    "\n",
    "prompt += '\\n'\n",
    "\n",
    "prompt += \"\"\"```\"\"\"\n",
    "prompt += '\\n'\n",
    "prompt += info_str\n",
    "prompt += '\\n'\n",
    "prompt += \"\"\"```\"\"\"\n",
    "prompt += '\\n'\n",
    "\n",
    "prompt += \"\"\"Give Python code to train a classification model of the form\n",
    "\n",
    "$Pr(ret_{t+1} = v \\vert ret_{t} = v)$, \n",
    "\n",
    "where\n",
    "\n",
    "$v \\in \\{big\\_neg, neg, sideways, pos, big\\_pos\\}$\n",
    "\n",
    "Make sure pos. includes big pos.; likewise neg. includes big neg.\n",
    "\n",
    "Please give an overview of your modeling process at the beginning, before any code. Also state any assumptions and data treatment you make at the beginning.\n",
    "\n",
    "Train and evaluate your model using expanding training windows.\n",
    "The first training window should be one year long.\n",
    "The model should be retrained every day, and the window should expand by one day each time.\n",
    "The model should be evaluated on the next day's data.\n",
    "\n",
    "Use as evaluation metrics:\n",
    "- false positive rate for each class\n",
    "- false negative rate for each class\n",
    "- full confusion matrix for each class\n",
    "\n",
    "NB: Since we have a multiclass problem and only one sample per day, per-day false positive and false negative rates are not meaningful. Instead, we should compute cumulative metrics over time.\n",
    "\n",
    "As artifacts, please provide:\n",
    "- the rolling predictions in a table\n",
    "- chart or charts visualizing the rolling predictions\n",
    "- chart or charts visualizing the evaluation metrics over time\n",
    "\n",
    "Some coding conventions:\n",
    "- after any data manipulation, please print the shape of the data\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# - have the model do one-vs-all classification\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Assume X is already loaded with the specified columns and DatetimeIndex\n",
    "\n",
    "# Compute daily returns\n",
    "X['return'] = X['close'].pct_change()\n",
    "\n",
    "# Compute intraday return and volatility\n",
    "X['intraday_return'] = (X['close'] - X['open']) / X['open']\n",
    "X['volatility'] = (X['high'] - X['low']) / X['low']\n",
    "\n",
    "# Remove initial NaN value from pct_change\n",
    "X = X.dropna()\n",
    "\n",
    "# Define quantile thresholds for return classes\n",
    "quantiles = X['return'].quantile([0.1, 0.4, 0.6, 0.9])\n",
    "q10 = quantiles.loc[0.1]\n",
    "q40 = quantiles.loc[0.4]\n",
    "q60 = quantiles.loc[0.6]\n",
    "q90 = quantiles.loc[0.9]\n",
    "\n",
    "# Function to assign return classes\n",
    "def assign_class(ret, q10, q40, q60, q90):\n",
    "    if ret <= q10:\n",
    "        return 0  # big_neg\n",
    "    elif ret <= q40:\n",
    "        return 1  # neg\n",
    "    elif ret <= q60:\n",
    "        return 2  # sideways\n",
    "    elif ret <= q90:\n",
    "        return 3  # pos\n",
    "    else:\n",
    "        return 4  # big_pos\n",
    "\n",
    "# Assign return classes\n",
    "X['return_class'] = X['return'].apply(lambda x: assign_class(x, q10, q40, q60, q90))\n",
    "\n",
    "# Shift the target variable to represent ret_{t+1}\n",
    "X['target'] = X['return_class'].shift(-1)\n",
    "\n",
    "# Drop the last row with NaN target\n",
    "X = X.dropna()\n",
    "\n",
    "# Features and target variable\n",
    "features = ['return', 'intraday_return', 'volatility', 'volume']\n",
    "target = 'target'\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "dates = []\n",
    "\n",
    "# Start date after one year\n",
    "start_index = X.index.get_loc(X.index[0] + pd.DateOffset(years=1))\n",
    "\n",
    "# Loop over each day starting from the first day after one year\n",
    "for i in range(start_index, len(X)):\n",
    "    # Expanding window training data\n",
    "    train_data = X.iloc[:i]\n",
    "    test_data = X.iloc[i:i+1]\n",
    "\n",
    "    # Prepare training and testing sets\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data[target]\n",
    "\n",
    "    # Train the model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(y_pred[0])\n",
    "    true_labels.append(y_test.values[0])\n",
    "    dates.append(X.index[i])\n",
    "\n",
    "# Create a DataFrame for rolling predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'True_Label': true_labels,\n",
    "    'Prediction': predictions\n",
    "}).set_index('Date')\n",
    "\n",
    "# Display the rolling predictions table\n",
    "print(results_df.head())\n",
    "\n",
    "# Initialize lists to store cumulative metrics over time\n",
    "cumulative_conf_matrices = []\n",
    "cumulative_fpr = []\n",
    "cumulative_fnr = []\n",
    "\n",
    "# Initialize empty arrays to store cumulative true labels and predictions\n",
    "cumulative_true_labels = []\n",
    "cumulative_predictions = []\n",
    "\n",
    "for i in range(len(results_df)):\n",
    "    cumulative_true_labels.append(results_df['True_Label'].iloc[i])\n",
    "    cumulative_predictions.append(results_df['Prediction'].iloc[i])\n",
    "    # Compute confusion matrix up to current point\n",
    "    cm = confusion_matrix(cumulative_true_labels, cumulative_predictions, labels=[0,1,2,3,4])\n",
    "    cumulative_conf_matrices.append(cm)\n",
    "    # Compute per-class false positive rate and false negative rate\n",
    "    fpr = {}\n",
    "    fnr = {}\n",
    "    for cls in range(5):\n",
    "        # For class cls, compute FP, FN, TP, TN\n",
    "        tp = cm[cls, cls]\n",
    "        fp = cm[:, cls].sum() - tp\n",
    "        fn = cm[cls, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        fpr[cls] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr[cls] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    cumulative_fpr.append(fpr)\n",
    "    cumulative_fnr.append(fnr)\n",
    "\n",
    "# Plot False Positive Rate over time for each class\n",
    "plt.figure(figsize=(15, 5))\n",
    "for cls in range(5):\n",
    "    plt.plot(dates, [fpr[cls] for fpr in cumulative_fpr], label=f'Class {cls}')\n",
    "plt.legend()\n",
    "plt.title('False Positive Rate over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Plot False Negative Rate over time for each class\n",
    "plt.figure(figsize=(15, 5))\n",
    "for cls in range(5):\n",
    "    plt.plot(dates, [fnr[cls] for fnr in cumulative_fnr], label=f'Class {cls}')\n",
    "plt.legend()\n",
    "plt.title('False Negative Rate over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('False Negative Rate')\n",
    "plt.show()\n",
    "\n",
    "# Display the confusion matrix at the end\n",
    "cm = confusion_matrix(true_labels, predictions, labels=[0,1,2,3,4])\n",
    "print('Final Confusion Matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume true_labels and predictions are your true and predicted labels\n",
    "# And class_names is a list of class names corresponding to labels [0, 1, 2, 3, 4]\n",
    "class_names = ['big_neg', 'neg', 'sideways', 'pos', 'big_pos']\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "# Calculate the percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Create a DataFrame for better labeling in the heatmap\n",
    "import pandas as pd\n",
    "cm_df = pd.DataFrame(cm_percent, index=class_names, columns=class_names)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_df, annot=True, fmt='.2f', cmap='Blues')\n",
    "\n",
    "plt.title('Confusion Matrix as Heatmap with Percentages')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs all classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The columns of `X` are open, high, low, close, and volume; X has as `DatetimeIndex` with daily frequency named \\\"date.\\\"\"\"\"\n",
    "prompt += \"\\n\"\n",
    "prompt += \"\"\"The data go from {0} to {1}.\"\"\".format(X.index[0], X.index[-1])\n",
    "prompt += \"\\n\"\n",
    "prompt += \"\"\"The data have {0} rows and {1} columns.\"\"\".format(X.shape[0], X.shape[1]) \n",
    "prompt += \"\\n\"\n",
    "prompt += \"\"\"Here is the result of `X.describe()` and `X.info()`:\n",
    "\"\"\"\n",
    "\n",
    "prompt += \"\\n\"\n",
    "\n",
    "prompt += \"\"\"```\"\"\"\n",
    "prompt += '\\n'\n",
    "prompt += X.describe().to_string()\n",
    "prompt += '\\n'\n",
    "prompt += \"\"\"```\"\"\"\n",
    "\n",
    "prompt += '\\n'\n",
    "\n",
    "prompt += \"\"\"```\"\"\"\n",
    "prompt += '\\n'\n",
    "prompt += info_str\n",
    "prompt += '\\n'\n",
    "prompt += \"\"\"```\"\"\"\n",
    "prompt += '\\n'\n",
    "\n",
    "prompt += \"\"\"Give Python code to train a classification model of the form\n",
    "\n",
    "$Pr(ret_{t+1} = big_pos \\vert ret_{t} = v)$, \n",
    "\n",
    "where\n",
    "\n",
    "$v \\in \\{big\\_neg, neg, sideways, pos, big\\_pos\\}$\n",
    "\n",
    "Make sure pos. includes big pos.; likewise neg. includes big neg.\n",
    "Specifically, define big_pos as any return greater than the 90th percentile, \n",
    "and big_neg as any return less than the 10th percentile, pos as any return greater than the 0.001, and neg as any return less than 0\n",
    "\n",
    "Please give an overview of your modeling process at the beginning, before any code. Also state any assumptions and data treatment you make at the beginning.\n",
    "\n",
    "Train and evaluate your model using expanding training windows.\n",
    "The first training window should be one year long: train on the first calendar year of the data and make predictions for each day going forward.\n",
    "The model should be retrained every day, and the window should expand by one day each time.\n",
    "The model should be evaluated on the next day's data.\n",
    "\n",
    "Use as evaluation metrics:\n",
    "- false positive rate for the class\n",
    "- false negative rate for the class\n",
    "- confusion matrix for the class\n",
    "- AUC-ROC score for the class\n",
    "\n",
    "NB 1: This is a one-vs-all classification problem with the target class being 'big_pos'.\n",
    "NB 2: Since we have only one sample per day, per-day false positive and false negative rates are not meaningful on a per day basis. Instead, we should compute cumulative metrics over time.\n",
    "\n",
    "As artifacts, please provide:\n",
    "- the rolling predictions in a table\n",
    "- chart or charts visualizing the rolling predictions\n",
    "- chart or charts visualizing the evaluation metrics over time\n",
    "- visualize the confusion matrix as heatmap with percentages and class labels\n",
    "- AUROC curve\n",
    "\n",
    "Some coding conventions:\n",
    "- after any data manipulation, please print the shape of the data\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# - have the model do one-vs-all classification\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Assume X is already loaded with the specified columns and DatetimeIndex\n",
    "\n",
    "# Step 1: Compute daily returns based on closing prices\n",
    "X['return'] = X['close'].pct_change()\n",
    "print('After computing daily returns:', X.shape)\n",
    "\n",
    "# Step 2: Compute intraday return and volatility\n",
    "X['intraday_return'] = (X['close'] - X['open']) / X['open']\n",
    "X['volatility'] = (X['high'] - X['low']) / X['low']\n",
    "print('After computing intraday return and volatility:', X.shape)\n",
    "\n",
    "# Step 3: Remove initial NaN values resulting from pct_change\n",
    "X.dropna(inplace=True)\n",
    "print('After dropping NaN values:', X.shape)\n",
    "\n",
    "# Step 4: Define return classes based on specific criteria\n",
    "# Compute the 10th and 90th percentiles\n",
    "q10 = X['return'].quantile(0.10)\n",
    "q90 = X['return'].quantile(0.90)\n",
    "print('10th percentile (q10):', q10)\n",
    "print('90th percentile (q90):', q90)\n",
    "\n",
    "# Function to assign return classes\n",
    "def assign_class(ret, q10, q90):\n",
    "    if ret <= q10:\n",
    "        return 'big_neg'  # Class 0\n",
    "    elif ret < 0:\n",
    "        return 'neg'      # Class 1\n",
    "    elif ret < 0.001:\n",
    "        return 'sideways' # Class 2\n",
    "    elif ret < q90:\n",
    "        return 'pos'      # Class 3\n",
    "    else:\n",
    "        return 'big_pos'  # Class 4\n",
    "\n",
    "# Step 5: Assign return classes\n",
    "X['return_class'] = X['return'].apply(lambda x: assign_class(x, q10, q90))\n",
    "print('After assigning return classes:', X.shape)\n",
    "\n",
    "# Step 6: Map return classes to numerical values for modeling purposes\n",
    "class_mapping = {\n",
    "    'big_neg': 0,\n",
    "    'neg': 1,\n",
    "    'sideways': 2,\n",
    "    'pos': 3,\n",
    "    'big_pos': 4\n",
    "}\n",
    "X['return_class_num'] = X['return_class'].map(class_mapping)\n",
    "print('After mapping return classes to numbers:', X.shape)\n",
    "\n",
    "# Step 7: Create the target variable (shifted return_class)\n",
    "X['target_class'] = X['return_class'].shift(-1)\n",
    "X['target'] = X['target_class'].apply(lambda x: 1 if x == 'big_pos' else 0)\n",
    "print('After creating target variable:', X.shape)\n",
    "\n",
    "# Drop the last row with NaN target\n",
    "X.dropna(inplace=True)\n",
    "print('After dropping NaN target row:', X.shape)\n",
    "\n",
    "# Step 8: Prepare features and target variable\n",
    "features = ['return', 'intraday_return', 'volatility', 'volume', 'return_class_num']\n",
    "target = 'target'\n",
    "\n",
    "# Step 9: Initialize lists to store results\n",
    "dates = []\n",
    "true_labels = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "cumulative_true_labels = []\n",
    "cumulative_predictions = []\n",
    "cumulative_probabilities = []\n",
    "auc_scores = []\n",
    "\n",
    "# Start date after one year\n",
    "start_date = X.index[0] + pd.DateOffset(years=1)\n",
    "if start_date not in X.index:\n",
    "    start_date = X.index[X.index.get_loc(start_date, method='nearest')]\n",
    "start_index = X.index.get_loc(start_date)\n",
    "print('Training starts from:', start_date)\n",
    "\n",
    "# Step 10: Expanding window training and evaluation\n",
    "for i in range(start_index, len(X)-1):\n",
    "    # Expanding window training data\n",
    "    train_data = X.iloc[:i]\n",
    "    test_data = X.iloc[i:i+1]\n",
    "    \n",
    "    # Prepare training and testing sets\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data[target]\n",
    "    \n",
    "    # Handle class imbalance with class weights\n",
    "    model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store results\n",
    "    dates.append(X.index[i+1])  # Prediction is for next day\n",
    "    true_labels.append(y_test.values[0])\n",
    "    predictions.append(y_pred[0])\n",
    "    probabilities.append(y_prob[0])\n",
    "    cumulative_true_labels.append(y_test.values[0])\n",
    "    cumulative_predictions.append(y_pred[0])\n",
    "    cumulative_probabilities.append(y_prob[0])\n",
    "    \n",
    "    # Compute AUC-ROC score cumulatively\n",
    "    if len(set(cumulative_true_labels)) > 1:\n",
    "        auc = roc_auc_score(cumulative_true_labels, cumulative_probabilities)\n",
    "        auc_scores.append(auc)\n",
    "    else:\n",
    "        auc_scores.append(np.nan)  # Cannot compute AUC with only one class\n",
    "\n",
    "print('Completed rolling predictions.')\n",
    "\n",
    "# Step 11: Compute cumulative metrics over time\n",
    "false_positive_rates = []\n",
    "false_negative_rates = []\n",
    "\n",
    "for i in range(len(cumulative_true_labels)):\n",
    "    current_true_labels = cumulative_true_labels[:i+1]\n",
    "    current_predictions = cumulative_predictions[:i+1]\n",
    "    current_probabilities = cumulative_probabilities[:i+1]\n",
    "    \n",
    "    cm = confusion_matrix(\n",
    "        current_true_labels,\n",
    "        current_predictions,\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "    if cm.shape == (2,2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn = cm[0,0]\n",
    "        fp = cm[0,1]\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "    \n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    false_positive_rates.append(fpr)\n",
    "    false_negative_rates.append(fnr)\n",
    "\n",
    "print('Computed cumulative metrics.')\n",
    "\n",
    "# Step 12: Create DataFrame for rolling predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'True_Label': true_labels,\n",
    "    'Prediction': predictions,\n",
    "    'Probability': probabilities\n",
    "}).set_index('Date')\n",
    "\n",
    "print('Rolling predictions table:')\n",
    "print(results_df.head())\n",
    "\n",
    "# Step 13: Visualization\n",
    "\n",
    "# Plot rolling predictions\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(results_df.index, results_df['True_Label'], label='True Label', marker='o', linestyle='', alpha=0.7)\n",
    "plt.plot(results_df.index, results_df['Prediction'], label='Predicted Label', marker='x', linestyle='', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Rolling Predictions over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Label (1=big_pos, 0=other)')\n",
    "plt.show()\n",
    "\n",
    "# Plot evaluation metrics over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(results_df.index, false_positive_rates, label='False Positive Rate')\n",
    "plt.plot(results_df.index, false_negative_rates, label='False Negative Rate')\n",
    "plt.legend()\n",
    "plt.title('Evaluation Metrics over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "# Plot AUC-ROC score over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(results_df.index, auc_scores, label='AUC-ROC Score')\n",
    "plt.legend()\n",
    "plt.title('AUC-ROC Score over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AUC-ROC Score')\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Visualize confusion matrix as heatmap\n",
    "# Compute final confusion matrix\n",
    "final_cm = confusion_matrix(\n",
    "    cumulative_true_labels,\n",
    "    cumulative_predictions,\n",
    "    labels=[0, 1]\n",
    ")\n",
    "\n",
    "# Calculate percentages\n",
    "cm_sum = final_cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_percent = final_cm.astype('float') / cm_sum * 100\n",
    "cm_percent = np.nan_to_num(cm_percent)  # Handle division by zero\n",
    "\n",
    "# Create DataFrame for heatmap\n",
    "cm_df = pd.DataFrame(\n",
    "    cm_percent,\n",
    "    index=['Other', 'big_pos'],\n",
    "    columns=['Predicted Other', 'Predicted big_pos']\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap with Percentages')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n",
    "\n",
    "# Step 15: Plot AUROC curve\n",
    "fpr, tpr, thresholds = roc_curve(cumulative_true_labels, cumulative_probabilities)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_scores[-1]:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\regin\\.conda\\envs\\d2lai\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:42:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Assume X is already loaded with the specified columns and DatetimeIndex\n",
    "\n",
    "# Step 1: Compute daily returns based on closing prices\n",
    "X['return'] = X['close'].pct_change()\n",
    "print('After computing daily returns:', X.shape)\n",
    "\n",
    "# Step 2: Compute intraday return and volatility\n",
    "X['intraday_return'] = (X['close'] - X['open']) / X['open']\n",
    "X['volatility'] = (X['high'] - X['low']) / X['low']\n",
    "print('After computing intraday return and volatility:', X.shape)\n",
    "\n",
    "# Step 3: Remove initial NaN values resulting from pct_change\n",
    "X.dropna(inplace=True)\n",
    "print('After dropping NaN values:', X.shape)\n",
    "\n",
    "# Step 4: Define return classes based on specific criteria\n",
    "# Compute the 10th and 90th percentiles\n",
    "q10 = X['return'].quantile(0.10)\n",
    "q90 = X['return'].quantile(0.90)\n",
    "print('10th percentile (q10):', q10)\n",
    "print('90th percentile (q90):', q90)\n",
    "\n",
    "# Function to assign return classes\n",
    "def assign_class(ret, q10, q90):\n",
    "    if ret <= q10:\n",
    "        return 'big_neg'  # Class 0\n",
    "    elif ret < 0:\n",
    "        return 'neg'      # Class 1\n",
    "    elif ret < 0.001:\n",
    "        return 'sideways' # Class 2\n",
    "    elif ret < q90:\n",
    "        return 'pos'      # Class 3\n",
    "    else:\n",
    "        return 'big_pos'  # Class 4\n",
    "\n",
    "# Step 5: Assign return classes\n",
    "X['return_class'] = X['return'].apply(lambda x: assign_class(x, q10, q90))\n",
    "print('After assigning return classes:', X.shape)\n",
    "\n",
    "# Step 6: Map return classes to numerical values for modeling purposes\n",
    "class_mapping = {\n",
    "    'big_neg': 0,\n",
    "    'neg': 1,\n",
    "    'sideways': 2,\n",
    "    'pos': 3,\n",
    "    'big_pos': 4\n",
    "}\n",
    "X['return_class_num'] = X['return_class'].map(class_mapping)\n",
    "print('After mapping return classes to numbers:', X.shape)\n",
    "\n",
    "# Step 7: Create the target variable (shifted return_class)\n",
    "X['target_class'] = X['return_class'].shift(-1)\n",
    "X['target'] = X['target_class'].apply(lambda x: 1 if x == 'big_pos' else 0)\n",
    "print('After creating target variable:', X.shape)\n",
    "\n",
    "# Drop the last row with NaN target\n",
    "X.dropna(inplace=True)\n",
    "print('After dropping NaN target row:', X.shape)\n",
    "\n",
    "# Step 8: Prepare features and target variable\n",
    "features = ['return', 'intraday_return', 'volatility', 'volume', 'return_class_num']\n",
    "target = 'target'\n",
    "\n",
    "# Step 9: Initialize lists to store results\n",
    "dates = []\n",
    "true_labels = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "cumulative_true_labels = []\n",
    "cumulative_predictions = []\n",
    "cumulative_probabilities = []\n",
    "auc_scores = []\n",
    "\n",
    "# Start date after one year\n",
    "start_date = X.index[0] + pd.DateOffset(years=1)\n",
    "if start_date not in X.index:\n",
    "    start_date = X.index[X.index.get_loc(start_date, method='nearest')]\n",
    "start_index = X.index.get_loc(start_date)\n",
    "print('Training starts from:', start_date)\n",
    "\n",
    "# Step 10: Expanding window training and evaluation\n",
    "for i in range(start_index, len(X)-1):\n",
    "    # Expanding window training data\n",
    "    train_data = X.iloc[:i]\n",
    "    test_data = X.iloc[i:i+1]\n",
    "    \n",
    "    # Prepare training and testing sets\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[target]\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data[target]\n",
    "    \n",
    "    # Calculate scale_pos_weight for handling class imbalance\n",
    "    num_negative = (y_train == 0).sum()\n",
    "    num_positive = (y_train == 1).sum()\n",
    "    if num_positive == 0:\n",
    "        scale_pos_weight = 1\n",
    "    else:\n",
    "        scale_pos_weight = num_negative / num_positive\n",
    "\n",
    "    # Define XGBoost classifier\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store results\n",
    "    dates.append(X.index[i+1])  # Prediction is for next day\n",
    "    true_labels.append(y_test.values[0])\n",
    "    predictions.append(y_pred[0])\n",
    "    probabilities.append(y_prob[0])\n",
    "    cumulative_true_labels.append(y_test.values[0])\n",
    "    cumulative_predictions.append(y_pred[0])\n",
    "    cumulative_probabilities.append(y_prob[0])\n",
    "    \n",
    "    # Compute AUC-ROC score cumulatively\n",
    "    if len(set(cumulative_true_labels)) > 1:\n",
    "        auc = roc_auc_score(cumulative_true_labels, cumulative_probabilities)\n",
    "        auc_scores.append(auc)\n",
    "    else:\n",
    "        auc_scores.append(np.nan)  # Cannot compute AUC with only one class\n",
    "\n",
    "print('Completed rolling predictions.')\n",
    "\n",
    "# Step 11: Compute cumulative metrics over time\n",
    "false_positive_rates = []\n",
    "false_negative_rates = []\n",
    "\n",
    "for i in range(len(cumulative_true_labels)):\n",
    "    current_true_labels = cumulative_true_labels[:i+1]\n",
    "    current_predictions = cumulative_predictions[:i+1]\n",
    "    current_probabilities = cumulative_probabilities[:i+1]\n",
    "    \n",
    "    cm = confusion_matrix(\n",
    "        current_true_labels,\n",
    "        current_predictions,\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "    if cm.shape == (2,2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn = cm[0,0]\n",
    "        fp = cm[0,1]\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "    \n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    false_positive_rates.append(fpr)\n",
    "    false_negative_rates.append(fnr)\n",
    "\n",
    "print('Computed cumulative metrics.')\n",
    "\n",
    "# Step 12: Create DataFrame for rolling predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'True_Label': true_labels,\n",
    "    'Prediction': predictions,\n",
    "    'Probability': probabilities\n",
    "}).set_index('Date')\n",
    "\n",
    "print('Rolling predictions table:')\n",
    "print(results_df.head())\n",
    "\n",
    "# Step 13: Visualization\n",
    "\n",
    "# Plot rolling predictions\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(results_df.index, results_df['True_Label'], label='True Label', marker='o', linestyle='', alpha=0.7)\n",
    "plt.plot(results_df.index, results_df['Prediction'], label='Predicted Label', marker='x', linestyle='', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Rolling Predictions over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Label (1=big_pos, 0=other)')\n",
    "plt.show()\n",
    "\n",
    "# Plot evaluation metrics over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(results_df.index, false_positive_rates, label='False Positive Rate')\n",
    "plt.plot(results_df.index, false_negative_rates, label='False Negative Rate')\n",
    "plt.legend()\n",
    "plt.title('Evaluation Metrics over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n",
    "\n",
    "# Plot AUC-ROC score over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(results_df.index, auc_scores, label='AUC-ROC Score')\n",
    "plt.legend()\n",
    "plt.title('AUC-ROC Score over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AUC-ROC Score')\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Visualize confusion matrix as heatmap\n",
    "# Compute final confusion matrix\n",
    "final_cm = confusion_matrix(\n",
    "    cumulative_true_labels,\n",
    "    cumulative_predictions,\n",
    "    labels=[0, 1]\n",
    ")\n",
    "\n",
    "# Calculate percentages\n",
    "cm_sum = final_cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_percent = final_cm.astype('float') / cm_sum * 100\n",
    "cm_percent = np.nan_to_num(cm_percent)  # Handle division by zero\n",
    "\n",
    "# Create DataFrame for heatmap\n",
    "cm_df = pd.DataFrame(\n",
    "    cm_percent,\n",
    "    index=['Other', 'big_pos'],\n",
    "    columns=['Predicted Other', 'Predicted big_pos']\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap with Percentages')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n",
    "\n",
    "# Step 15: Plot AUROC curve\n",
    "fpr, tpr, thresholds = roc_curve(cumulative_true_labels, cumulative_probabilities)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_scores[-1]:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
