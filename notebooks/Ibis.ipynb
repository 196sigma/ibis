{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibis\n",
    "Created: 09/13/2024\\\n",
    "Updated: 09/14/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import add_trading_hours\n",
    "from fredapi import Fred\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST SET COMPUTING ENVIRONMENT\n",
    "COMPUTING_ENV = 'windows'\n",
    "#COMPUTING_ENV = 'ubuntu'\n",
    "#COMPUTING_ENV = 'aws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTING_ENV == 'windows':\n",
    "    WORKING_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\ibis\"\n",
    "    API_KEYS_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\API_KEYS\"\n",
    "elif COMPUTING_ENV == 'ubuntu':\n",
    "    WORKING_DIR = \"/home/reggie//Dropbox/ibis\"\n",
    "    API_KEYS_DIR = \"/home/reggie/Dropbox/API_KEYS\"\n",
    "elif COMPUTING_ENV == 'aws':\n",
    "    WORKING_DIR = \"/home/ubuntu/ibis\"\n",
    "    API_KEYS_DIR = \"/home/ubuntu/API_KEYS\"\n",
    "\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "STOCK_DATA_DIR  = os.path.join(DATA_DIR, 'tmp')\n",
    "FRD_DATA_DIR = os.path.join(DATA_DIR, 'frd-historical')\n",
    "print(f\"Working directory is\\n\\t{WORKING_DIR}\")\n",
    "print(f\"Data directory is\\n\\t{DATA_DIR}\")\n",
    "print(f\"Stock data directory is\\n\\t{STOCK_DATA_DIR}\")\n",
    "print(f\"FRD data directory is\\n\\t{FRD_DATA_DIR}\")\n",
    "\n",
    "# data dictionary\n",
    "data_dictionary_fp = os.path.join(DATA_DIR, \"data_dictionary.json\")\n",
    "if not os.path.exists(data_dictionary_fp):\n",
    "    print(f\"Data dictionary does not exist in {data_dictionary_fp}. Initializing empty data dictionary.\")\n",
    "    data_dictionary = {}\n",
    "else:\n",
    "    print(f\"Data dictionary exists in {data_dictionary_fp}. Loading data dictionary.\")\n",
    "    with open(data_dictionary_fp, \"r\") as f:\n",
    "        data_dictionary = json.load(f)\n",
    "    print('\\tKeys:', data_dictionary.keys())\n",
    "\n",
    "# OpenAI API key\n",
    "openai_api_key_fp = os.path.join(API_KEYS_DIR, 'openai-api-key-1.txt')\n",
    "with open(openai_api_key_fp) as f:\n",
    "    OPENAI_API_KEY = f.read().strip()\n",
    "print(f\"OpenAI API key is {OPENAI_API_KEY}\")\n",
    "\n",
    "# FRED Data\n",
    "fred_api_key_fp = os.path.join(API_KEYS_DIR, 'FRED-API-KEY')\n",
    "with open(fred_api_key_fp) as f:\n",
    "    fred_api_key = f.read().strip()\n",
    "print(f\"FRED API key is {fred_api_key}\")\n",
    "FRED_DIR = os.path.join(DATA_DIR, \"FRED\")\n",
    "fred_daily_data_fp = os.path.join(FRED_DIR, 'daily', 'FRED_daily.csv')\n",
    "if not os.path.exists(fred_daily_data_fp):\n",
    "    print(f\"FRED daily data do not exist in {fred_daily_data_fp}.\")\n",
    "else:\n",
    "    print(f\"FRED daily data are in {fred_daily_data_fp}\")\n",
    "\n",
    "# a table to map asset types to download directories\n",
    "frd_download_directories = pd.read_csv(os.path.join(FRD_DATA_DIR, 'frd-download-directories.csv'))\n",
    "frd_download_directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_prices(ticker, asset_type, period='full', timeframe='1min', adjustment='adj_splitdiv', data_dir='./'):\n",
    "    if asset_type == 'stock':\n",
    "        csv_filename = f\"{ticker}_{period}_{timeframe}_{adjustment.replace('_','')}.txt\"\n",
    "        fp = os.path.join(data_dir, csv_filename)\n",
    "        prices_df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\",\",\n",
    "            names=['date', 'open', 'high', 'low', 'close', 'volume'],\n",
    "            header=0,  # Assuming the first row is a header, if not set to None\n",
    "            on_bad_lines='warn',  # Skip bad lines\n",
    "            engine='python'  # Use the Python engine for more flexible error handling\n",
    "        )\n",
    "    elif asset_type == 'index':\n",
    "        csv_filename = f\"{ticker}_{period}_{timeframe}.txt\"\n",
    "        fp = os.path.join(data_dir, csv_filename)\n",
    "        prices_df = pd.read_csv(\n",
    "            fp,\n",
    "            sep=\",\",\n",
    "            names=['date', 'open', 'high', 'low', 'close',],\n",
    "            header=0,  # Assuming the first row is a header, if not set to None\n",
    "            on_bad_lines='warn',  # Skip bad lines\n",
    "            engine='python'  # Use the Python engine for more flexible error handling\n",
    "        )\n",
    "\n",
    "    # Convert 'date' column to datetime if it's not already\n",
    "    prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "    \n",
    "    # Extract the day as YYYY-MM-DD\n",
    "    prices_df['day'] = prices_df['date'].dt.date\n",
    "    \n",
    "    if period in ['1min', '5min', '30min', '1hour']:\n",
    "        # Extract the time as HH:MM:SS\n",
    "        prices_df['time'] = prices_df['date'].dt.time\n",
    "        \n",
    "        # Calculate the time ID (minute of the day from 1 to 1440)\n",
    "        prices_df['time_id'] = prices_df['date'].dt.hour * 60 + prices_df['date'].dt.minute + 1\n",
    "    \n",
    "        prices_df.set_index('date', inplace=True)\n",
    "        prices_df = add_trading_hours(prices_df)\n",
    "    else:\n",
    "        prices_df.set_index('date', inplace=True)\n",
    "\n",
    "    # add returns\n",
    "    prices_df.sort_index(inplace=True, ascending=True)\n",
    "    prices_df['open_to_close_ret'] = prices_df['close']/prices_df['open'] - 1\n",
    "    prices_df['close_to_close_ret'] = prices_df['close'].pct_change()\n",
    "    prices_df['overnight_ret'] = prices_df['open']/prices_df['close'].shift(1) - 1\n",
    "    prices_df['open_to_high_ret'] = prices_df['high']/prices_df['open'] - 1\n",
    "    prices_df['open_to_low_ret'] = prices_df['low']/prices_df['open'] - 1\n",
    "    prices_df['low_to_high'] = prices_df['high']/prices_df['low'] - 1 # max possible return\n",
    "\n",
    "    prices_df['ticker'] = ticker\n",
    "    \n",
    "    return prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_type = 'index'\n",
    "ticker = 'SPX'\n",
    "period = 'full'\n",
    "timeframe = '1day'\n",
    "index_data_dir = os.path.join(FRD_DATA_DIR, frd_download_directories.query(f\"type == '{asset_type}' & timeframe == '{timeframe}'\")['directory'].values[0], 'csv')\n",
    "print(f\"Index data directory is {index_data_dir}\")\n",
    "spx_df = load_stock_prices(\n",
    "    ticker=ticker,\n",
    "    asset_type=asset_type,\n",
    "    period=period,\n",
    "    timeframe=timeframe,\n",
    "    data_dir=index_data_dir\n",
    ")\n",
    "spx_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_from_gpt_markdown(content: str, save: bool = False, filename: str = None):\n",
    "    \"\"\"This function takes in a ChatGPT response that is in Markdown form with Python code blocks and returns the Python code as a string.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        content : str : The ChatGPT response in Markdown form\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        python_str : str : The Python code as a string\n",
    "    \"\"\"\n",
    "    python_str = \"\"\n",
    "    in_python_block = False\n",
    "    for line in content.split(\"\\n\"):\n",
    "        if line.startswith(\"```python\"):\n",
    "            in_python_block = True\n",
    "        elif line.startswith(\"```\"):\n",
    "            in_python_block = False\n",
    "        elif in_python_block:\n",
    "            python_str += line + \"\\n\"\n",
    "    if save:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(python_str)\n",
    "    return python_str\n",
    "\n",
    "def gpt_code(system_prompt: str, user_prompt: str, filename, save, model='gpt-4-o') -> (str, str):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return content, parse_python_from_gpt_markdown(content, save=False, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downoad FRED Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key=fred_api_key)\n",
    "\n",
    "fred_tips_series = ['DFII10', 'DFII5', 'DFII20', 'DFII30',]\n",
    "fred_treasury_series = ['DGS10', 'DGS2', 'DGS30']\n",
    "\n",
    "fred_tips_daily_data_fp = os.path.join(FRED_DIR, 'daily', 'FRED_daily_tips.csv')\n",
    "fred_treasuries_daily_data_fp = os.path.join(FRED_DIR, 'daily', 'FRED_daily_treasuries.csv')\n",
    "\n",
    "create_or_update_fred_tips_data = False\n",
    "create_or_update_fred_treasuries_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treasury Inflation-Indexed Securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if create_or_update_fred_tips_data:\n",
    "\n",
    "    # create or update data dictionary\n",
    "    for series in fred_tips_series:\n",
    "        if series not in data_dictionary:\n",
    "            data_dictionary[series] = dict(fred.get_series_info(series))\n",
    "        else:\n",
    "            print(f\"{series} already in data dictionary.\")\n",
    "\n",
    "    # export data_dictionary to json\n",
    "    with open(data_dictionary_fp, 'w') as f:\n",
    "        json.dump(data_dictionary, f, indent=2)\n",
    "\n",
    "    tips_df = pd.DataFrame({\n",
    "        series: fred.get_series(series) for series in fred_tips_series\n",
    "    })\n",
    "    tips_df.reset_index(inplace=True)\n",
    "    tips_df.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "    tips_df['date'] = pd.to_datetime(tips_df['date'])\n",
    "    tips_df.set_index('date', inplace=True)\n",
    "    tips_df.to_csv(fred_tips_daily_data_fp)\n",
    "else:\n",
    "    tips_df = pd.read_csv(fred_tips_daily_data_fp, index_col='date', parse_dates=True)\n",
    "tips_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treasury Constant Maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_or_update_fred_treasuries_data:\n",
    "    for series in fred_treasury_series:\n",
    "        if series not in data_dictionary:\n",
    "            data_dictionary[series] = dict(fred.get_series_info(series))\n",
    "        else:\n",
    "            print(f\"{series} already in data dictionary.\")\n",
    "            \n",
    "    # export data_dictionary to json\n",
    "    with open(data_dictionary_fp, 'w') as f:\n",
    "        json.dump(data_dictionary, f, indent=2)\n",
    "\n",
    "    treasuries_df = pd.DataFrame({\n",
    "        series: fred.get_series(series) for series in fred_treasury_series\n",
    "    })\n",
    "    treasuries_df.reset_index(inplace=True)\n",
    "    treasuries_df.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "    treasuries_df['date'] = pd.to_datetime(treasuries_df['date'])\n",
    "    treasuries_df.set_index('date', inplace=True)\n",
    "    treasuries_df.to_csv(fred_treasuries_daily_data_fp)\n",
    "else:\n",
    "    treasuries_df = pd.read_csv(fred_treasuries_daily_data_fp, index_col='date', parse_dates=True)\n",
    "treasuries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Inflation and Interest Rate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Series and Plot Levels and Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_tips_daily_df = pd.read_csv(fred_tips_daily_data_fp, index_col='date', parse_dates=True)\n",
    "X = fred_tips_daily_df.copy()\n",
    "X.ffill(inplace=True)\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "# add pct change\n",
    "X_pct_change = X.pct_change()\n",
    "X_pct_change.dropna(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in X.columns:\n",
    "    plt.plot(X.index, X[col], label=col)\n",
    "plt.title(\"Treasury Inflation-Indexed Securities (TIPS)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in X_pct_change.columns:\n",
    "    plt.plot(X_pct_change.index, X_pct_change[col], label=col)\n",
    "plt.title(\"Treasury Inflation-Indexed Securities (TIPS) Percent Change\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_pct_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_pct_change.corr()\n",
    "print(corr.round(2))\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "print(corr.round(2))\n",
    "\n",
    "# heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD on daily treasuries\n",
    "U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# s contains the singular values (variance explained by each component)\n",
    "# U contains the left singular vectors (temporal patterns)\n",
    "# Vt contains the right singular vectors (relationships between variables)\n",
    "\n",
    "# Retain only top k components for dimensionality reduction\n",
    "k = 3\n",
    "U_k = U[:, :k]\n",
    "S_k = np.diag(s[:k])\n",
    "Vt_k = Vt[:k, :]\n",
    "\n",
    "# Low-rank approximation of X\n",
    "X_approx = np.dot(U_k, np.dot(S_k, Vt_k))\n",
    "\n",
    "# Singular values show the importance of each component\n",
    "print(\"Top singular values:\", s[:10])\n",
    "\n",
    "# Use Vt_k to analyze relationships between variables (columns of X)\n",
    "print(\"Right singular vectors (V^T):\\n\", Vt_k)\n",
    "\n",
    "# first two SVs as factors\n",
    "factors = U_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot (Singular Values)\n",
    "A scree plot displays the magnitude of the singular values, which tells you how much variance each component explains. This can help identify how many components are important and where the diminishing returns occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 's' contains the singular values from SVD\n",
    "def plot_scree(s):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(s) + 1), s, marker='o', linestyle='-')\n",
    "    plt.title(\"Scree Plot of Singular Values\")\n",
    "    plt.xlabel(\"Component Number\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_scree(s[:50])  # Plot the first 50 singular values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Variance Explained\n",
    "\n",
    "Another useful plot is the cumulative explained variance, which shows how much total variance is explained as you include more singular values. It helps to decide how many components are necessary to capture most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_variance(s):\n",
    "    explained_variance = np.cumsum(s**2) / np.sum(s**2)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(s) + 1), explained_variance, marker='o', linestyle='-')\n",
    "    plt.title(\"Cumulative Explained Variance\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_cumulative_variance(s[:50])  # Plot cumulative variance for the first 50 components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Top Singular Vectors (Right Singular Vectors $V^T$)\n",
    "\n",
    "You can visualize the right singular vectors, which describe how the variables (columns of the original data) contribute to each principal component. A heatmap can highlight the relationship between variables and components.\n",
    "\n",
    "This heatmap shows the contribution of each time series (variables) to the top principal components. Patterns, correlations, and groups of similar variables will be visible, showing how variables are related to each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_singular_vectors(Vt, k=10):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(Vt[:k, :], cmap='coolwarm', center=0)\n",
    "    plt.title(f\"Heatmap of Top {k} Right Singular Vectors (V^T)\")\n",
    "    plt.xlabel(\"Variables (Time Series)\")\n",
    "    plt.ylabel(\"Singular Vector Index\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_singular_vectors(Vt_k, k=10)  # Plot top 10 right singular vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Projections (Left Singular Vectors U)\n",
    "\n",
    "The left singular vectors represent temporal patterns. You can project the original time series data onto the top components and visualize these projections to understand the key dynamics of the system over time.\n",
    "\n",
    "This plot shows how the time series evolve in terms of the most important components. You can spot trends, cycles, or other important temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_projections(U, components=[0, 1]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for comp in components:\n",
    "        plt.plot(U[:, comp], label=f\"Component {comp+1}\")\n",
    "    \n",
    "    plt.title(\"Time Series Projections onto Top Components\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Projection\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_time_series_projections(U_k, components=[0, 1])  # Plot the first two components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D/3D Scatter Plot of Time Series in Reduced Space\n",
    "\n",
    "By projecting the time series into a reduced space (using the top singular vectors), you can visualize the relationships between time steps or time series in lower dimensions (e.g., a 2D or 3D scatter plot).\n",
    "\n",
    "These projections show how the data points cluster in lower dimensions, helping to identify groups, trends, or anomalies. In 2D or 3D, clusters or separations between groups of time steps or variables may become more evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_projection(U, Vt, singular_values):\n",
    "    # Project the data onto the first two components\n",
    "    projection = np.dot(U[:, :2], np.diag(singular_values[:2]))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(projection[:, 0], projection[:, 1], alpha=0.5, s=10)\n",
    "    plt.title(\"2D Projection of Time Series Data\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_2d_projection(U_k, Vt_k, s[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_projection(U, singular_values):\n",
    "    # Project the data onto the first three components\n",
    "    projection = np.dot(U[:, :3], np.diag(singular_values[:3]))\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(projection[:, 0], projection[:, 1], projection[:, 2], alpha=0.5, s=10)\n",
    "    \n",
    "    ax.set_title(\"3D Projection of Time Series Data\")\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_3d_projection(U_k, s[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Rates II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips_df.merge(treasuries_df, left_index=True, right_index=True, suffixes=('_tips', '_treasuries'))\n",
    "# rename 'date' index to 'time_index'\n",
    "X.index.rename('time_index', inplace=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of minutes in a year\n",
    "minutes_in_year = 525600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "n = 100_000 # number of minutes\n",
    "X = np.random.randn(n, k)\n",
    "time_index = pd.date_range(start='2020-01-01', periods=len(X), freq='T')  # 'T' for minute frequency\n",
    "X = pd.DataFrame(X, index=time_index).round(2)\n",
    "X.columns = [f\"feature_{i}\" for i in range(k)]\n",
    "print(X.shape)\n",
    "X.head()\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import cwt, ricker\n",
    "from scipy.fft import fft, fftfreq\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('./figures', exist_ok=True)\n",
    "os.makedirs('./tables', exist_ok=True)\n",
    "\n",
    "# Assume your dataframe is X\n",
    "# Replace 'target_column_name' with your actual target column name\n",
    "target_col = 'feature_0'\n",
    "target_series = X[target_col]\n",
    "\n",
    "# List of temporal resolutions to examine\n",
    "# Resample to coarser frequencies\n",
    "resolutions = ['5T', '15T', '30T', '1H', '1D']  # 5 Minutes, 15 Minutes, 30 Minutes, Hourly, Daily\n",
    "\n",
    "for res in resolutions:\n",
    "    print(f\"\\nAnalyzing data resampled to {res} resolution.\")\n",
    "    \n",
    "    # Resample the data\n",
    "    X_resampled = X.resample(res).mean()\n",
    "    target_resampled = X_resampled[target_col]\n",
    "    \n",
    "    # Save resampled target series\n",
    "    target_resampled.to_csv(f'./tables/target_series_{res}.csv')\n",
    "    \n",
    "    # 1. Correlation Analysis at the current resolution\n",
    "    correlations = X_resampled.corrwith(target_resampled).sort_values(ascending=False)\n",
    "    top_correlations = correlations.head(10)\n",
    "    print(f\"Top correlated series with the target series at {res} resolution:\")\n",
    "    print(top_correlations)\n",
    "    \n",
    "    # Save top correlations to a CSV file\n",
    "    top_correlations.to_csv(f'./tables/top_correlated_series_{res}.csv', header=['Correlation'])\n",
    "    \n",
    "    # Plot the top correlated series with the target series\n",
    "    top_correlated_series = correlations.index[1:6]  # Exclude the first one (itself)\n",
    "    for col in top_correlated_series:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(target_resampled.index, target_resampled.values, label='Target Series')\n",
    "        plt.plot(target_resampled.index, X_resampled[col].values, label=f'Series {col}')\n",
    "        plt.legend()\n",
    "        plt.title(f'Target Series vs Series {col} at {res} resolution')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        # Save the plot\n",
    "        plt.savefig(f'./figures/target_vs_series_{col}_{res}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Stationarity Tests (ADF Test)\n",
    "    adf_result = adfuller(target_resampled.dropna())\n",
    "    adf_output = pd.Series(adf_result[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    print(f\"ADF Test Result for Target Series at {res} resolution:\")\n",
    "    print(adf_output)\n",
    "    \n",
    "    # Save ADF test result\n",
    "    adf_output.to_csv(f'./tables/adf_test_{res}.csv', header=['Value'])\n",
    "    \n",
    "    # 3. Time Series Decomposition\n",
    "    # Since STL requires a frequency, we need to set it based on the resampled data\n",
    "    # For example, if resampling to '1H', the period might be 24 (hours in a day)\n",
    "    freq_dict = {'5T': 288, '15T': 96, '30T': 48, '1H': 24, '1D': 7}  # Adjusted periods\n",
    "    freq = freq_dict[res]\n",
    "    stl = STL(target_resampled.dropna(), period=freq)\n",
    "    stl_result = stl.fit()\n",
    "    \n",
    "    # Plot the decomposition\n",
    "    stl_result.plot()\n",
    "    plt.suptitle(f'STL Decomposition of Target Series at {res} resolution')\n",
    "    plt.savefig(f'./figures/stl_decomposition_{res}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save decomposition components\n",
    "    decomposition_df = pd.DataFrame({\n",
    "        'Trend': stl_result.trend,\n",
    "        'Seasonal': stl_result.seasonal,\n",
    "        'Residual': stl_result.resid\n",
    "    }, index=target_resampled.dropna().index)\n",
    "    decomposition_df.to_csv(f'./tables/stl_decomposition_{res}.csv')\n",
    "    \n",
    "    # 4. Spectral Analysis (Fourier Transform)\n",
    "    n = len(target_resampled.dropna())\n",
    "    yf = fft(target_resampled.dropna().values)  # Convert Series to NumPy array\n",
    "    xf = fftfreq(n, 1)[:n//2]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(xf, 2.0/n * np.abs(yf[0:n//2]))\n",
    "    plt.title(f'Frequency Domain of Target Series at {res} resolution')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.savefig(f'./figures/frequency_domain_{res}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save frequency and amplitude data\n",
    "    freq_ampl_df = pd.DataFrame({'Frequency': xf, 'Amplitude': 2.0/n * np.abs(yf[0:n//2])})\n",
    "    freq_ampl_df.to_csv(f'./tables/frequency_domain_{res}.csv', index=False)\n",
    "    \n",
    "    # 5. Wavelet Transform (Time-Frequency Analysis)\n",
    "    widths = np.arange(1, 31)\n",
    "    cwt_matr = cwt(target_resampled.fillna(0).values, ricker, widths)  # Convert Series to NumPy array\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(cwt_matr, extent=[0, len(target_resampled), 1, 31], cmap='PRGn', aspect='auto',\n",
    "               vmax=abs(cwt_matr).max(), vmin=-abs(cwt_matr).max())\n",
    "    plt.title(f'Continuous Wavelet Transform of Target Series at {res} resolution')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Scale')\n",
    "    plt.savefig(f'./figures/cwt_{res}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Note: Saving the entire wavelet matrix may not be practical due to size\n",
    "    \n",
    "    # 6. PCA at the current resolution\n",
    "    pca = PCA(n_components=5)\n",
    "    principal_components = pca.fit_transform(X_resampled.fillna(0))\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    print(f\"Explained Variance Ratios by Principal Components at {res} resolution:\")\n",
    "    for i, ev in enumerate(explained_variance):\n",
    "        print(f\"PC{i+1}: {ev:.4f}\")\n",
    "    \n",
    "    # Save explained variance ratios\n",
    "    ev_df = pd.DataFrame({'Explained Variance Ratio': explained_variance},\n",
    "                         index=[f'PC{i+1}' for i in range(len(explained_variance))])\n",
    "    ev_df.to_csv(f'./tables/pca_explained_variance_{res}.csv')\n",
    "    \n",
    "    # Correlate PCs with target series\n",
    "    pc_df = pd.DataFrame(principal_components, index=X_resampled.index, columns=[f'PC{i+1}' for i in range(5)])\n",
    "    pc_df[target_col] = target_resampled.values\n",
    "    pc_correlations = pc_df.corr()[target_col][:-1]  # Exclude the target column itself\n",
    "    print(f\"Correlation between Target Series and Principal Components at {res} resolution:\")\n",
    "    print(pc_correlations)\n",
    "    \n",
    "    # Save PC correlations with target series\n",
    "    pc_correlations.to_csv(f'./tables/pc_correlations_with_target_{res}.csv', header=['Correlation'])\n",
    "    \n",
    "    # Plot the target series with principal components\n",
    "    for pc in pc_df.columns[:-1]:  # Exclude the target column\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(target_resampled.index, target_resampled.values, label='Target Series')\n",
    "        plt.plot(target_resampled.index, pc_df[pc].values, label=pc)\n",
    "        plt.legend()\n",
    "        plt.title(f'Target Series vs {pc} at {res} resolution')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        # Save the plot\n",
    "        plt.savefig(f'./figures/target_vs_{pc}_{res}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. Autocorrelation and Partial Autocorrelation Plots at the current resolution\n",
    "    # Determine the maximum number of lags based on data length\n",
    "    max_lags = min(20, len(target_resampled.dropna()) - 1)\n",
    "    \n",
    "    if max_lags > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plot_acf(target_resampled.dropna(), lags=max_lags)\n",
    "        plt.title(f'Autocorrelation Function of Target Series at {res} resolution')\n",
    "        plt.savefig(f'./figures/target_series_acf_{res}.png')\n",
    "        plt.close()\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plot_pacf(target_resampled.dropna(), lags=max_lags)\n",
    "        plt.title(f'Partial Autocorrelation Function of Target Series at {res} resolution')\n",
    "        plt.savefig(f'./figures/target_series_pacf_{res}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"Not enough data to compute autocorrelation at {res} resolution.\")\n",
    "    \n",
    "    # Optional: Granger Causality Tests at the current resolution\n",
    "    max_lag = 10\n",
    "    test_results = {}\n",
    "    for col in top_correlated_series:\n",
    "        data = X_resampled[[target_col, col]].dropna()\n",
    "        if len(data) > max_lag:\n",
    "            test = grangercausalitytests(data, maxlag=max_lag, verbose=False)\n",
    "            p_values = [round(test[i+1][0]['ssr_chi2test'][1], 4) for i in range(max_lag)]\n",
    "            test_results[col] = p_values\n",
    "        else:\n",
    "            print(f\"Not enough data for Granger Causality Test at {res} resolution for series {col}.\")\n",
    "            test_results[col] = [np.nan]*max_lag\n",
    "    \n",
    "    gc_df = pd.DataFrame(test_results, index=range(1, max_lag+1))\n",
    "    print(f\"Granger Causality Test P-values at {res} resolution:\")\n",
    "    print(gc_df)\n",
    "    \n",
    "    # Save Granger causality test results\n",
    "    gc_df.to_csv(f'./tables/granger_causality_pvalues_{res}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RDFN Price Data\n",
    "\n",
    "* Log(Price) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "ticker = 'RDFN'\n",
    "asset_type = \"stock\"        # Example values: stock, etf, futures, crypto, index, fx\n",
    "period = \"full\"             # Example values: full, month, week, day\n",
    "timeframe = \"1min\"          # Example values: 1min, 5min, 30min, 1hour, 1day\n",
    "adjustment = \"adj_splitdiv\"    # Example values: adj_split, adj_splitdiv, UNADJUSTED\n",
    "stock_csv_filename = f\"{ticker}_{period}_{timeframe}_{adjustment.replace('_','')}.txt\"\n",
    "print(stock_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rdfn_df = load_stock_prices(ticker, asset_type, period, timeframe, adjustment, STOCK_DATA_DIR)\n",
    "\n",
    "print(rdfn_df.info())\n",
    "rdfn_df[['open', 'high', 'low', 'close', 'volume']] = rdfn_df[['open', 'high', 'low', 'close', 'volume']].apply(np.log)\n",
    "rdfn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt\n",
    "rdfn_df = pd.read_csv(os.path.join(DATA_DIR, stock_csv_filename.replace('.txt', '.csv')), parse_dates=True, index_col='date')\n",
    "rdfn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfn_df = rdfn_df.query(\"date >= '2024-07-01'\")\n",
    "rdfn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfn_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot high\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rdfn_df['high'].values, label=f\"{ticker} High\")\n",
    "plt.title(f\"{ticker} High Price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# line plot volume\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rdfn_df['volume'].values, label=f\"{ticker} Volume\")\n",
    "plt.title(f\"{ticker} Volume\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily resampling\n",
    "rdfn_daily_df = rdfn_df.resample('D').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "})\n",
    "rdfn_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = pd.read_csv(os.path.join(FRED_DIR, 'daily', 'FRED_daily_tips.csv'), parse_dates=True, index_col='date')\n",
    "tips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treasuries_df = pd.read_csv(os.path.join(FRED_DIR, 'daily', 'FRED_daily_treasuries.csv'), parse_dates=True, index_col='date')\n",
    "treasuries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_df = tips_df.join(treasuries_df, how='inner')\n",
    "macro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rdfn_daily_df.join(macro_df, how='left')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.query(\"volume > 0\").query(\"date >= '2024-09-01'\").query(\"date < '2024-09-13'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(X.index, X['DFII10'], label='DFII10')\n",
    "# title\n",
    "plt.title(\"Treasury Inflation-Indexed Securities (TIPS)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
