{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = None # must set to \"local\" or \"remote\" before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11234db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import urllib\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#from configs import EXT_DATA_DIR, FRD_USER_ID, FRD_DATA_DIR\n",
    "# configs\n",
    "\n",
    "# ask for environment via input\n",
    "if ENVIRONMENT is None:\n",
    "    ENVIRONMENT = input(\"Please enter environment (local/remote): \")\n",
    "    assert ENVIRONMENT in [\"local\", \"remote\"], \"Invalid environment\"\n",
    "\n",
    "with open(FRD_USER_ID_FP, \"r\") as file:\n",
    "    FRD_USER_ID = file.read().strip()\n",
    "if ENVIRONMENT == \"local\":\n",
    "    if os.name == \"nt\":\n",
    "        PROJECT_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\ibis\"\n",
    "        FRD_DATA_DIR = \"E:\\\\frd-historical\\\\data\\\\\"\n",
    "        FRD_USER_ID_FP = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\API_KEYS\\\\FRD-USER-ID\"\n",
    "    else:\n",
    "        PROJECT_DIR = \"/home/reggie/Dropbox/ibis/\"\n",
    "        FRD_DATA_DIR = \"/media/reggie/reg_ext/frd-historical/data/\"\n",
    "        FRD_USER_ID_FP = \"/home/reggie/Dropbox/API_KEYS/FRD-USER-ID\"\n",
    "elif ENVIRONMENT == \"remote\":\n",
    "    PROJECT_DIR = \"/home/ubuntu/ibis/\"\n",
    "    FRD_DATA_DIR = \"/home/ubuntu/ibis/frd-historical/data/\"\n",
    "    FRD_USER_ID_FP = \"/home/ubuntu/FRD-USER-ID\"\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, \"data\")\n",
    "#FRD_DATA_DIR = os.path.join(DATA_DIR, \"frd-historical\")\n",
    "\n",
    "# Trading calendar, holidays\n",
    "calendar = {\n",
    "    \"july4\":[\"2023-07-03\"],\n",
    "    \"911\":[\"2023-09-11\"],\n",
    "    \"thanksgiving\":[\"2023-11-24\"],\n",
    "    \"xmas\":[\"2023-12-25\"],\n",
    "    \"nyd\":[\"2023-01-01\"],\n",
    "}\n",
    "\n",
    "print(FRD_DATA_DIR, os.listdir(FRD_DATA_DIR))\n",
    "print(DATA_DIR, os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f46d1a",
   "metadata": {},
   "source": [
    "# Ticker Listing\n",
    "* Get available stock (for now) ticker symbols from FRD.\n",
    "* Store the sheet in `save_fp = os.path.join(FRD_DATA_DIR, \"stock\", \"ticker_listing.csv\")`.\n",
    "* Add a column if it seems that the stock was delisted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ticker Listing\n",
    "This function returns the full listing of tickers as well as start and end dates for a specified instrument type.\n",
    "The data is returned in csv format so it can be copied into a text file and opened from a spreadsheet application if required.\n",
    "\n",
    "Url EndPoint : https://firstratedata.com/api/ticker_listing\n",
    "\n",
    "Data Format : {ticker},{name},{startDate},{endDate}\n",
    "\n",
    "Parameters : The below parameters are used with the Url Endpoint to use the Last Update function:\n",
    "\n",
    "Parameter : type\n",
    "Accepted Values : stock , etf(only stocks or ETFs can currently be requested)\n",
    "\n",
    "Description : Specifies the type of instrument that is being requested.\n",
    "\n",
    "Example :\n",
    "https://firstratedata.com/api/ticker_listing?type=stock&userid=c85lvfWKHUivhqC3fDJlBw\n",
    "\n",
    "Parameter : html\n",
    "Accepted Values : true, false (false is the default value)\n",
    "\n",
    "Description : Specifies the is the returned data is in HTML format. Set this value to true to view the data in a web browser.\n",
    "\n",
    "Example :\n",
    "https://firstratedata.com/api/ticker_listing?type=stock&userid=c85lvfWKHUivhqC3fDJlBw&html=true\n",
    "\"\"\"\n",
    "\n",
    "def ticker_listing(type, save_fp=\"\", html=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    type : string\n",
    "        Accepted Values : stock , etf(only stocks or ETFs can currently be requested)\n",
    "        Description : Specifies the type of instrument that is being requested.\n",
    "\n",
    "    html : boolean\n",
    "        Accepted Values : true, false (false is the default value)\n",
    "        Description : Specifies the is the returned data is in HTML format. Set this value to true to view the data in a web browser.\n",
    "    \"\"\"\n",
    "    url = \"https://firstratedata.com/api/ticker_listing?type={}&userid={}&html={}\".format(type, FRD_USER_ID, html)\n",
    "\n",
    "    # download\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, save_fp)\n",
    "        print(\"Downloaded ticker listing to {}\".format(save_fp))\n",
    "\n",
    "        ticker_listing_df = pd.read_csv(save_fp)\n",
    "        ticker_listing_df.columns = [\"ticker\", \"name\", \"first_date\", \"last_date\"]\n",
    "\n",
    "        # Apply the function to the 'last_date' column to create the 'delisted' flag\n",
    "        # TODO: Improve because this has edge cases with zero volume trading days\n",
    "        ticker_listing_df['delisted'] = ticker_listing_df['last_date'].apply(is_delisted)\n",
    "        ticker_listing_df.to_csv(save_fp, index=False)\n",
    "        return 0\n",
    "    except:\n",
    "        print(\"Error: Could not download ticker listing.\")\n",
    "\n",
    "    # save ticker listing\n",
    "\n",
    "    return\n",
    "\n",
    "def is_delisted(last_date_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Function to check if the last_date is before the most recent weekday.\n",
    "    Doesn't necessarily require a str input, could be a datetime object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    last_date_str : string\n",
    "        Last date of the stock data    \n",
    "    \"\"\"\n",
    "    last_date = datetime.strptime(last_date_str, '%Y-%m-%d').date()\n",
    "    today = datetime.now().date() - timedelta(days=7)\n",
    "    # Adjust today to the most recent weekday if it's a weekend\n",
    "    if today.weekday() > 4:  # 0 is Monday, 6 is Sunday\n",
    "        today -= timedelta(days=today.weekday() - 4)\n",
    "    return int(last_date < today)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a139af-066c-44e2-862a-f09aa21d416b",
   "metadata": {},
   "source": [
    "# Querying the First Rate Data Historical Stock Database API\n",
    "\n",
    "## Process\n",
    "<ol> \n",
    "<li>Download to ext </li>\n",
    "<li>Compress on ext</li>\n",
    "<li>move to local HD</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "## Download list\n",
    "* full series for 1min, 5min, 30min, 1hour, and 1day for stocks, etfs, index, crypto, fx, futures, and options\n",
    "* include (separately) splitdiv adjusted, split adjusted, and unadjusted\n",
    "* Get dividends and splits \n",
    "\n",
    "\n",
    "## Directory Structure\n",
    "* foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ca538",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fp = os.path.join(FRD_DATA_DIR, \"stock\", \"ticker_listing.csv\")\n",
    "#ticker_listing(\"stock\", save_fp=save_fp,)\n",
    "ticker_listing_df = pd.read_csv(save_fp)\n",
    "print(ticker_listing_df.shape)\n",
    "ticker_listing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_listing_df['ticker_first_letter'] = ticker_listing_df['ticker'].str[0]\n",
    "ticker_listing_df.sort_values('ticker_first_letter', inplace=True)\n",
    "ticker_first_letter_counts = ticker_listing_df['ticker_first_letter'].value_counts().reset_index()\n",
    "ticker_first_letter_counts.sort_values(\"ticker_first_letter\", inplace=True)\n",
    "ticker_first_letter_counts\n",
    "plt.bar(ticker_first_letter_counts['ticker_first_letter'], ticker_first_letter_counts['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281f7bd-d2e3-483d-8fad-fb8e48042112",
   "metadata": {},
   "source": [
    "# Historical Data\n",
    "See https://firstratedata.com/about/api-docs?userid=c85lvfWKHUivhqC3fDJlBw#datarequest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6902b8",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ba284",
   "metadata": {},
   "source": [
    "### Download SPY S&P 500 Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example downloading SPY\n",
    "ticker = \"SPY\"\n",
    "ticker_first_letter = ticker[0]\n",
    "type = \"etf\"\n",
    "period = \"full\"\n",
    "timeframe = \"1min\"\n",
    "adjustment = \"adj_split\"\n",
    "base_url = 'https://firstratedata.com/api/data_file'\n",
    "\n",
    "frd_api_params = {\n",
    "    'type': type,\n",
    "    'period': period,\n",
    "    'ticker_range': ticker_first_letter,\n",
    "    'timeframe': timeframe,\n",
    "    'adjustment': adjustment,\n",
    "    'userid': FRD_USER_ID\n",
    "}\n",
    "download_filename = \"temp/tmp.zip\"\n",
    "response = requests.get(base_url, params=frd_api_params)\n",
    "with open(download_filename, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "extract_zip(download_filename, f\"temp/etfs/{ticker_first_letter}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82966ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjustment = adjustment.replace(\"_\", \"\")\n",
    "spy_filepath = f\"temp/etfs/{ticker_first_letter}/{ticker}_{period}_{timeframe}_{adjustment}.txt\"\n",
    "column_names = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "print(spy_filepath)\n",
    "spy_df = pd.read_csv(spy_filepath, header=None, names=column_names, parse_dates=[\"date\"], index_col=\"date\")\n",
    "print(spy_df.shape)\n",
    "spy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bab17f",
   "metadata": {},
   "source": [
    "### Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4186adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if windows or linux\n",
    "if os.name == 'nt':\n",
    "    print('Windows')\n",
    "    ZIP_FILES_DIR = os.path.join(EXT_DATA_DIR, \"stock\", timeframe, \"zips\")\n",
    "else:\n",
    "    print('Linux')\n",
    "    ZIP_FILES_DIR = f\"/media/reggie/reg_ext/frd-historical/data/stock/{timeframe}/zips/\"\n",
    "    \n",
    "def make_download_path(**params):\n",
    "    \"\"\"\n",
    "    This is for First Rate Data downloads (zip files.)\n",
    "    Make a download path for the given parameters. \n",
    "    If the path does not exist, create it.\n",
    "    \"\"\"\n",
    "    adjustment = params['adjustment']\n",
    "    timeframe = params['timeframe']\n",
    "    ticker_first_letter = params['ticker_first_letter']    \n",
    "    \n",
    "    fp = os.path.join(\n",
    "        ZIP_FILES_DIR,\n",
    "        adjustment,\n",
    "        timeframe,\n",
    "    )\n",
    "    if ticker_first_letter:\n",
    "        fp = os.path.join(fp, ticker_first_letter)\n",
    "    if not os.path.exists(fp):\n",
    "        print(\"making directory {}\".format(fp))\n",
    "        os.makedirs(fp)\n",
    "    return fp\n",
    "\n",
    "def download_stock_data(**params):\n",
    "    \"\"\"\n",
    "    Download stock data from FirstRateData API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ticker_first_letter : string\n",
    "        First letter of the ticker symbol to download data for.\n",
    "    timeframe : string\n",
    "        Timeframe to download data for. Accepted values are: 1min, 5min, 30min, 1hour, 1day\n",
    "    adjustment : string\n",
    "        Adjustment type to download data for. Accepted values are: adj_split, adj_splitdiv, unadjusted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> download_stock_data('A', '1min', 'adj_split')\n",
    "    Downloaded: data/stock/A/adj_split/1min/A_adj_split_1min.zip\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    asset_type = params['asset_type']\n",
    "    period = params['period']\n",
    "    ticker_first_letter = params['ticker_first_letter']\n",
    "    timeframe = params['timeframe']\n",
    "    adjustment = params['adjustment']\n",
    "    overwrite = params['overwrite']\n",
    "\n",
    "    ZIP_FILES_DIR = f\"/media/reggie/reg_ext/frd-historical/data/stock/{timeframe}/zips/\"\n",
    "    if not os.path.exists(ZIP_FILES_DIR):\n",
    "        os.makedirs(ZIP_FILES_DIR)\n",
    "        \n",
    "    # Define API URL\n",
    "    base_url = 'https://firstratedata.com/api/data_file'\n",
    "\n",
    "    frd_api_params = {\n",
    "        'type': asset_type,\n",
    "        'period': period,\n",
    "        'ticker_range': ticker_first_letter,\n",
    "        'timeframe': timeframe,\n",
    "        'adjustment': adjustment,\n",
    "        'userid': FRD_USER_ID\n",
    "    }\n",
    "    print(ticker_first_letter)\n",
    "    filename = f\"{ticker_first_letter}_{adjustment}_{timeframe}.zip\"\n",
    "    zips_filename = os.path.join(ZIP_FILES_DIR, filename)\n",
    "    print(zips_filename)\n",
    "\n",
    "    #download_path = make_download_path(**params)\n",
    "    download_filename = os.path.join(ZIP_FILES_DIR, filename)\n",
    "    if os.path.exists(zips_filename) and overwrite is False:\n",
    "        print(f\"Already downloaded {ticker_first_letter} {period} data for {timeframe} and {adjustment} to {download_filename}\")\n",
    "    else:\n",
    "        if not os.path.exists(zips_filename) or overwrite is True:\n",
    "            print(f\"Downloading {ticker_first_letter} {period} data for {timeframe} and {adjustment} to {download_filename}\")\n",
    "            response = requests.get(base_url, params=frd_api_params)\n",
    "            if response.status_code == 200:\n",
    "                with open(download_filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded: {download_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {ticker_first_letter} data for {timeframe} and {adjustment}. Status: {response.status_code}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb06b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    \"asset_type\": \"stock\",\n",
    "    \"ticker_first_letter\": \"Z\",\n",
    "    #\"timeframe\": \"5min\",\n",
    "    \"timeframe\": \"1min\",\n",
    "    \"adjustment\": \"adj_splitdiv\",\n",
    "    \"period\": \"full\",\n",
    "    \"overwrite\": True,\n",
    "    'userid': FRD_USER_ID\n",
    "    }\n",
    "download_stock_data(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d99de",
   "metadata": {},
   "source": [
    "### Download All ticker ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd05830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate A-Z\n",
    "EXT_ZIP_FILES_DIR = \"/media/reggie/reg_ext/frd-historical/data/stock/zips/\"\n",
    "def download_stock_data_batch(ticker_start_letters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n",
    "                              timeframes=['1min', '5min', '30min', '1hour', '1day'],\n",
    "                              adjustments=['adj_split', 'adj_splitdiv', 'unadjusted'],\n",
    "                              overwrite=True,\n",
    "                              move=False,\n",
    "                              move_dir=None,\n",
    "):\n",
    "\n",
    "    # Iterate through tickers and download data\n",
    "    for ticker_first_letter in ticker_start_letters:\n",
    "        print(f\"Downloading data for {ticker_first_letter}\")\n",
    "        for adjustment in adjustments:\n",
    "            for timeframe in timeframes:\n",
    "                if timeframe != '1min' and adjustment == 'unadjusted':\n",
    "                    continue\n",
    "                \n",
    "                # Create params.\n",
    "                params = {\n",
    "                    \"asset_type\": \"stock\",\n",
    "                    \"ticker_first_letter\": ticker_first_letter,\n",
    "                    \"timeframe\": timeframe,\n",
    "                    \"adjustment\": adjustment,\n",
    "                    \"period\": \"full\",\n",
    "                    \"overwrite\": overwrite,\n",
    "                    'userid': FRD_USER_ID\n",
    "                }\n",
    "\n",
    "                # Download.\n",
    "                download_stock_data(**params)\n",
    "                filename = f\"{ticker_first_letter}_{adjustment}_{timeframe}.zip\"\n",
    "                download_path = os.path.join(ZIP_FILES_DIR, filename)\n",
    "                print(\"Download path\", download_path)\n",
    "                \n",
    "                fp_size = os.path.getsize(download_path)\n",
    "                print(f\"Downloaded {ticker_first_letter} {timeframe} {adjustment} data into file {download_path} with size {fp_size/(2**20)} MB\")\n",
    "                \n",
    "                if move is True:\n",
    "                    print(f\"Zip file to move: {filename}\")\n",
    "                    move_path = os.path.join(move_dir, filename)\n",
    "                    print(f\"Moving {download_path} to {move_path}\")\n",
    "                    # move and overwrite if necessary\n",
    "                    shutil.move(download_path, move_path)\n",
    "    return 0\n",
    "download_stock_data_batch(\n",
    "    ticker_start_letters=\"QX\",\n",
    "    timeframes=['1min'],\n",
    "    adjustments=['adj_splitdiv'],\n",
    "    overwrite=True,\n",
    "    move=True,\n",
    "    move_dir=EXT_ZIP_FILES_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcaf5d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2418f447",
   "metadata": {},
   "source": [
    "## Extract stock files from zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f163cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(src_path, dest_dir):\n",
    "    \"\"\"\n",
    "    Extract a .zip file to a directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_path : string\n",
    "        Path to the .zip file to extract.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dest_dir : string\n",
    "        Path to the directory where the .txt files were extracted.\n",
    "    \"\"\"\n",
    "    #dest_dir = os.path.join(FRD_DATA_DIR, \"tmp\")\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    print(f\"Extracting {src_path} to {dest_dir}\")\n",
    "    with zipfile.ZipFile(src_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(path=dest_dir)\n",
    "    n_files = len(os.listdir(dest_dir))\n",
    "    print(f\"Extracted {n_files} files from {src_path}\")\n",
    "    return dest_dir\n",
    "\n",
    "def extract_stock_zips(**params):\n",
    "    \"\"\"Extract all .zip files for the given parameters.\"\"\"\n",
    "    # Unpack parameters\n",
    "    adjustment = params['adjustment']\n",
    "    timeframe = params['timeframe']\n",
    "    ticker_first_letter = params['ticker_first_letter']\n",
    "    \n",
    "    ZIP_FILES_DIR = f\"/media/reggie/reg_ext/frd-historical/data/stock/{timeframe}/zips/\"\n",
    "    CSV_DEST_DIR = f\"/media/reggie/reg_ext/frd-historical/data/stock/{timeframe}/csv/\"\n",
    "\n",
    "    # Find zip files\n",
    "    print(f\"Found {len(os.listdir(ZIP_FILES_DIR))} zip files in {ZIP_FILES_DIR}\")\n",
    "    zip_path = os.path.join(ZIP_FILES_DIR, f\"{ticker_first_letter}_{adjustment}_{timeframe}.zip\")\n",
    "    print(CSV_DEST_DIR)\n",
    "    if not os.path.isdir(CSV_DEST_DIR):\n",
    "        os.makedirs(CSV_DEST_DIR)\n",
    "        print(f\"Created directory {CSV_DEST_DIR}\")\n",
    "    \n",
    "    # Extract\n",
    "    src_path = extract_zip(src_path=zip_path, dest_dir=CSV_DEST_DIR)\n",
    "    print(src_path)\n",
    "\n",
    "    files = [f for f in os.listdir(src_path) if f.endswith('.txt')]\n",
    "    print(f\"Number of files: {len(files)} in {src_path}\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#asset_type = \"stock\"\n",
    "#ticker_first_letter = 'Q'\n",
    "#timeframe = \"1min\"\n",
    "#adjustment = \"adj_splitdiv\"\n",
    "#period = \"full\"\n",
    "#overwrite = True\n",
    "#params = {\n",
    "#    \"asset_type\": asset_type,\n",
    "#    \"ticker_first_letter\": ticker_first_letter,\n",
    "#    \"timeframe\": timeframe,\n",
    "#    \"adjustment\": adjustment,\n",
    "#    \"period\": period,\n",
    "#    \"overwrite\": overwrite\n",
    "#}\n",
    "extract_stock_zips(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40439cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_type = \"stock\"\n",
    "timeframe = \"1min\"\n",
    "adjustment = \"adj_splitdiv\"\n",
    "period = \"full\"\n",
    "overwrite = True\n",
    "#for ticker_first_letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "for ticker_first_letter in \"BCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "    params = {\n",
    "        \"asset_type\": asset_type,\n",
    "        \"ticker_first_letter\": ticker_first_letter,\n",
    "        \"timeframe\": timeframe,\n",
    "        \"adjustment\": adjustment,\n",
    "        \"period\": period,\n",
    "        \"overwrite\": overwrite\n",
    "    }\n",
    "    extract_stock_zips(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import add\n",
    "\n",
    "\n",
    "def get_csv_stats(file_path):\n",
    "    \"\"\"Get the number of rows and min, max number of columns of a csv file\"\"\"\n",
    "    csv_stats = {'n_rows':0, 'min_columns':0, 'max_columns':0}\n",
    "    columns_counts = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "    \n",
    "    for line in contents:\n",
    "        columns_counts.append(len(line.split(',')))\n",
    "    csv_stats['n_rows'] = len(contents)\n",
    "    csv_stats['min_columns'] = min(columns_counts)\n",
    "    csv_stats['max_columns'] = max(columns_counts)\n",
    "    return csv_stats \n",
    "\n",
    "def get_directory_stats(directory, add_stats=True):\n",
    "    \"\"\"\n",
    "    Get the file size of each file in a directory.\n",
    "    Return it as a pandas dataframe.    \n",
    "    \"\"\"\n",
    "    file_stats = []\n",
    "    for file in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_stats.append({\n",
    "            \"file_path\": file_path,\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"file_size\": file_size/(2**20)\n",
    "        })\n",
    "        if add_stats is True:\n",
    "            # add columns on row and column stats\n",
    "            csv_stats = get_csv_stats(file_path)\n",
    "            file_stats[-1].update(csv_stats)\n",
    "\n",
    "    file_stats_df = pd.DataFrame(file_stats)\n",
    "    \n",
    "    return file_stats_df\n",
    "csv_stats_df = get_directory_stats(directory=CSV_DEST_DIR, add_stats=False)\n",
    "csv_stats_df.sort_values(\"file_size\", ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ac53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# shuffle csv_stats_df\n",
    "csv_stats_df = csv_stats_df.sample(frac=1)\n",
    "n = len(csv_stats_df)\n",
    "df_list = []\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    row = csv_stats_df.iloc[i]\n",
    "    file = row['file_path']\n",
    "    filename = row['filename']\n",
    "    #print(file)\n",
    "    df = row.to_dict() \n",
    "    df.update(get_csv_stats(file))\n",
    "    df = pd.DataFrame(df, index=[0])\n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list)\n",
    "df.to_csv(os.path.join(FRD_DATA_DIR, \"stock_csv_stats.csv\"), index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76427abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"max_columns != 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dac1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "# List of all first letters to process\n",
    "ticker_first_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Use ThreadPoolExecutor to execute tasks in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Create a list of futures\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            extract_stock_zips(\n",
    "                asset_type=\"stock\",\n",
    "                ticker_first_letter=ticker_first_letter,\n",
    "                timeframe=\"1min\",\n",
    "                adjustment=\"adj_splitdiv\",\n",
    "                period=\"full\",\n",
    "                overwrite=True,)\n",
    "        ) for ticker_first_letter in ticker_first_letters\n",
    "    ]\n",
    "    \n",
    "    # Wait for all futures to complete\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            # Get the result of the future\n",
    "            result = future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        else:\n",
    "            # Process result (if necessary)\n",
    "            print(f\"Task completed with result: {result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc57708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combine all files in the list\n",
    "columns = [\"ticker\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "data = [\",\".join(columns)+\"\\n\"]\n",
    "print(f\"Writing to {csv_dest_path}\")\n",
    "with open(csv_dest_path, 'w') as f:\n",
    "    f.writelines(data)\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    ticker = file.split('_')[0]\n",
    "    file_path = os.path.join(src_path, file)\n",
    "    print(f\"Processing file {i+1}/{len(files)}: {file}\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [ticker + ',' + line for line in f.readlines()]\n",
    "        print(len(data))\n",
    "\n",
    "        # append to csv\n",
    "        print(f\"Writing to {csv_dest_path}\")\n",
    "        with open(csv_dest_path, 'a') as f:\n",
    "            f.writelines(data)\n",
    "\n",
    "# convert dataframe to parquet\n",
    "df = pd.read_csv(csv_dest_path, dtype={'ticker': str, 'date': str, 'open': float, 'high': float, 'low': float, 'close': float, 'volume': float})\n",
    "print(df.shape)\n",
    "parquet_fp = csv_dest_path.replace(\".csv\", \".parquet\")\n",
    "print(parquet_fp)\n",
    "df.to_parquet(parquet_fp, index=False)\n",
    "\n",
    "# clean up: remove individual .txt files\n",
    "for file in files:\n",
    "    os.remove(os.path.join(src_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a525949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0ace31",
   "metadata": {},
   "source": [
    "## reorg directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_data():\n",
    "    base_path = \"data/stock\"\n",
    "    adjustments = ['adj_split', 'adj_splitdiv', 'unadjusted']\n",
    "    timeframes = ['1min', '5min', '30min', '1hour', '1day']\n",
    "    \n",
    "    for letter_dir in os.listdir(base_path):\n",
    "        current_path = os.path.join(base_path, letter_dir)\n",
    "        # Skip if not a directory\n",
    "        if not os.path.isdir(current_path):\n",
    "            continue\n",
    "        \n",
    "        for adj in adjustments:\n",
    "            for timeframe in timeframes:\n",
    "                # Define the source path for current adjustment and timeframe\n",
    "                src_path = os.path.join(current_path, adj, timeframe)\n",
    "                if not os.path.exists(src_path):\n",
    "                    continue\n",
    "                \n",
    "                # Move each zip file into a new structure based on its name\n",
    "                for file in os.listdir(src_path):\n",
    "                    ticker_symbol = file.split('_')[0]  # Assuming filename format is \"Symbol_adj_timeframe.zip\"\n",
    "                    new_path = os.path.join(base_path, ticker_symbol, adj, timeframe)\n",
    "                    if not os.path.exists(new_path):\n",
    "                        os.makedirs(new_path)\n",
    "                    \n",
    "                    # Move file from old structure to new structure\n",
    "                    shutil.move(os.path.join(src_path, file), os.path.join(new_path, file))\n",
    "                \n",
    "                # Cleanup: remove the now-empty directories\n",
    "                if not os.listdir(src_path):\n",
    "                    os.rmdir(src_path)\n",
    "            # Check and remove empty adjustment directory\n",
    "            adj_path = os.path.join(current_path, adj)\n",
    "            if not os.listdir(adj_path):\n",
    "                os.rmdir(adj_path)\n",
    "                \n",
    "        # Check and remove empty letter directory\n",
    "        if not os.listdir(current_path):\n",
    "            os.rmdir(current_path)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    reorganize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c61214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc5343b5",
   "metadata": {},
   "source": [
    "# Load data into database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stand up database for extracted txt files\n",
    "# create table for each ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb45bca",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"RDFN\"\n",
    "filename = [f for f in files if f.find(ticker) != -1][0]\n",
    "fp = os.path.join(ZIP_SRC_FP, filename)\n",
    "print(fp)\n",
    "#prices_df = pd.read_csv(fp, sep=\",\")\n",
    "prices_df = pd.read_csv(fp, sep=\",\", names=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "print(prices_df.shape)\n",
    "prices_df['date'] = prices_df['date'].apply(pd.to_datetime)\n",
    "\n",
    "prices_df['ticker'] = ticker\n",
    "prices_df['day'] = prices_df['date'].apply(lambda d: d.day)\n",
    "prices_df['month'] = prices_df['date'].apply(lambda d: d.month)\n",
    "prices_df['hour'] = prices_df['date'].apply(lambda d: d.hour)\n",
    "prices_df['minute'] = prices_df['date'].apply(lambda d: d.minute)\n",
    "prices_df['period'] = prices_df['month'].apply(lambda x: str(x).zfill(2)) + \\\n",
    "    prices_df['day'].apply(lambda x: str(x).zfill(2)) + \\\n",
    "    prices_df['hour'].apply(lambda x: str(x).zfill(2)) + \\\n",
    "    prices_df['minute'].apply(lambda x: str(x).zfill(2))\n",
    "prices_df['period'] = prices_df['period'].astype(int)\n",
    "prices_df.describe()\n",
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac710a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "month, day = 10, 23\n",
    "for day in range(1, 24):\n",
    "    X = prices_df.query(f\"month == {month}\").query(f\"day == {day}\")\n",
    "    if len(X) > 0:\n",
    "        X['open_scaled'] = X['open']/X['open'].values[0]\n",
    "        plt.plot(X['period'], X['open_scaled'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe177a91",
   "metadata": {},
   "source": [
    "# Overnight returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d94ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_open(dt, buffer=0):\n",
    "    dt = pd.to_datetime(dt)\n",
    "    return np.all((dt.hour == 9, dt.minute == 30 + buffer))\n",
    "\n",
    "def is_close(dt, buffer=0):\n",
    "    dt = pd.to_datetime(dt)\n",
    "    return np.all((dt.hour == 16, dt.minute == 0 - buffer))\n",
    "prices_df['at_open'] = prices_df['date'].apply(lambda x: is_open(x,1))\n",
    "prices_df['at_close'] = prices_df['date'].apply(lambda x: is_close(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157610c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overnight_df = prices_df.query(\"(at_close == True) | (at_open == True)\").sort_values('date')\n",
    "overnight_df['open_lag1'] = overnight_df['open'].shift(1)\n",
    "overnight_df['close_lag1'] = overnight_df['close'].shift(1)\n",
    "overnight_df['delta_price'] = overnight_df['open'] - overnight_df['close_lag1']\n",
    "overnight_df['lret'] = np.log(overnight_df['open']/overnight_df['close_lag1'])\n",
    "overnight_df[['date','period', 'close_lag1', 'open', 'delta_price', 'lret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "X = overnight_df[['date', 'lret']].reset_index()\n",
    "X = X.assign(index=list(range(len(X))))\n",
    "plt.bar(x=X['index'], height=X['lret'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
