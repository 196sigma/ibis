{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTING_ENV = 'aws' # must set to \"local\" or \"remote\" before running\n",
    "COMPUTING_ENV = 'ubuntu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTING_ENV == 'windows':\n",
    "    WORKING_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\ibis\"\n",
    "    API_KEYS_DIR = \"C:\\\\Users\\\\regin\\\\Dropbox\\\\API_KEYS\"\n",
    "elif COMPUTING_ENV == 'ubuntu':\n",
    "    WORKING_DIR = \"/home/reggie//Dropbox/ibis\"\n",
    "    API_KEYS_DIR = \"/home/reggie/Dropbox/API_KEYS\"\n",
    "elif COMPUTING_ENV == 'aws':\n",
    "    WORKING_DIR = \"/home/ubuntu/ibis\"\n",
    "    API_KEYS_DIR = \"/home/ubuntu/API_KEYS\"\n",
    "\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "FRD_DATA_DIR = os.path.join(DATA_DIR, 'frd-historical')\n",
    "\n",
    "frd_download_directories = pd.read_csv(os.path.join(FRD_DATA_DIR, 'frd-download-directories.csv'))\n",
    "frd_download_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframes = [\"1min\", \"5min\", \"daily\", \"weekly\", \"monthly\"]\n",
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "print(f\"Working directory:\\n\\t{WORKING_DIR}\")\n",
    "print(f\"Data directory:\\n\\t{DATA_DIR}\")\n",
    "print(f\"FRD data directory:\\n\\t{FRD_DATA_DIR}\")\n",
    "\n",
    "# make dirs\n",
    "for d in [DATA_DIR, FRD_DATA_DIR,]:\n",
    "    if not os.path.exists(d):\n",
    "        print(f\"Creating {d}\")\n",
    "        os.makedirs(d)\n",
    "\n",
    "\n",
    "\n",
    "FRD_USER_ID = open(os.path.join(API_KEYS_DIR, 'FRD-USER-ID')).read().strip()\n",
    "print(f\"FRD User ID: {FRD_USER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_frd_data(params, zips_dir, overwrite=False):\n",
    "    period = params['period']\n",
    "    adjustment = params['adjustment']\n",
    "    timeframe = params['timeframe']\n",
    "\n",
    "    if period == 'full':\n",
    "        ticker_range = params['ticker_range']\n",
    "        zip_fp = os.path.join(zips_dir, f\"{ticker_range}_{period}_{adjustment}_{timeframe}.zip\")\n",
    "    else:\n",
    "        zip_fp = os.path.join(zips_dir, f\"{period}_{adjustment}_{timeframe}.zip\")\n",
    "\n",
    "    if os.path.exists(zip_fp) and not overwrite:\n",
    "        print(f\"File already exists: {zip_fp}\")\n",
    "        return\n",
    "    \n",
    "    base_url = \"https://firstratedata.com/api/data_file\"\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    print(response.url)\n",
    "    if response.status_code == 200:      \n",
    "        with open(zip_fp, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"ZIP file saved: {zip_fp}\")\n",
    "    else:\n",
    "        print(f\"Failed to download data: {response.status_code}\")\n",
    "\n",
    "def extract_frd_zip(\n",
    "        src_zips_dir, \n",
    "        dest_csv_dir, \n",
    "        ticker_first_letter='', \n",
    "        period='full', \n",
    "        adjustment='adj_splitdiv', \n",
    "        timeframe='1min', \n",
    "        overwrite=False):\n",
    "    if period == 'full':\n",
    "        src_zip_fp = os.path.join(src_zips_dir, f\"{ticker_first_letter}_{period}_{adjustment}_{timeframe}.zip\")\n",
    "        dest_csv_fp = os.path.join(dest_csv_dir, ticker_first_letter)\n",
    "    else:\n",
    "        src_zip_fp = os.path.join(src_zips_dir, f\"{period}_{adjustment}_{timeframe}.zip\")\n",
    "        dest_csv_fp = dest_csv_dir\n",
    "    dest_dir_size = len(os.listdir(dest_csv_fp))\n",
    "\n",
    "    # extract if overwrite == True or directory is empty\n",
    "    if (dest_dir_size == 0) or overwrite:\n",
    "        if os.path.isfile(src_zip_fp):\n",
    "            print(f\"Extracting {src_zip_fp} to {dest_csv_fp}\")\n",
    "        else:\n",
    "            print(f\"File not found: {src_zip_fp}\")\n",
    "            return\n",
    "        with zipfile.ZipFile(src_zip_fp, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest_csv_fp)\n",
    "    else:\n",
    "        print(f\"{dest_csv_fp} not empty with {dest_dir_size} files\")\n",
    "        return\n",
    "    \n",
    "def make_zips_and_csv_dirs(params):\n",
    "    data_date = datetime.now().strftime('%Y%m%d')\n",
    "    adjustment = params['adjustment']\n",
    "    timeframe = params['timeframe']\n",
    "    period = params['period']\n",
    "    \n",
    "    _dir = stock_download_directories.query(f\"type == 'stock' & timeframe == '{timeframe}' & adjustment == '{adjustment}'\")['directory'].values[0]\n",
    "    \n",
    "    if period == 'day':\n",
    "        _dir = _dir.replace('stock', f'stock_day_{data_date}')\n",
    "    elif period == 'week':\n",
    "        _dir = _dir.replace('stock', f'stock_week_{data_date}')\n",
    "    elif period == 'month':\n",
    "        _dir = _dir.replace('stock', f'stock_month_{data_date}')\n",
    "    _dir = os.path.join(FRD_DATA_DIR, _dir)\n",
    "    zips_dir = os.path.join(_dir, 'zips')\n",
    "    csv_dir = os.path.join(_dir, 'csv')\n",
    "\n",
    "\n",
    "    return zips_dir, csv_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Full Stock History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Minute Stock Bars Adjusted for Splits and Dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"type\": \"stock\",\n",
    "    \"ticker_range\": \"\",\n",
    "    \"timeframe\": \"1min\",\n",
    "    \"adjustment\": \"adj_splitdiv\",\n",
    "    \"period\": \"full\",\n",
    "    'userid': FRD_USER_ID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIPS_DIR, CSV_DIR = make_zips_and_csv_dirs(params)\n",
    "print(f\"ZIPS_DIR: {ZIPS_DIR}\")\n",
    "print(f\"CSV_DIR: {CSV_DIR}\")\n",
    "\n",
    "if not os.path.exists(CSV_DIR):\n",
    "    print(f\"Creating {CSV_DIR}\")\n",
    "    os.makedirs(CSV_DIR)\n",
    "else:\n",
    "    print(f\"{CSV_DIR} exists with {len(os.listdir(CSV_DIR))} files\")\n",
    "if not os.path.exists(ZIPS_DIR):\n",
    "    print(f\"Creating {ZIPS_DIR}\")\n",
    "    os.makedirs(ZIPS_DIR)\n",
    "else:\n",
    "    print(f\"{ZIPS_DIR} exists with {len(os.listdir(ZIPS_DIR))} files\")\n",
    "\n",
    "for ticker_first_letter in letters:\n",
    "    ticker_csv_dir = os.path.join(CSV_DIR, ticker_first_letter)\n",
    "    print(ticker_csv_dir)\n",
    "    \n",
    "    if not os.path.exists(ticker_csv_dir):\n",
    "        os.makedirs(ticker_csv_dir)\n",
    "        print(f\"Creating {ticker_csv_dir}\")\n",
    "    else:\n",
    "        print(f\"{ticker_csv_dir} exists with {len(os.listdir(ticker_csv_dir))} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker_first_letter in tqdm(letters):\n",
    "    params['ticker_range'] = ticker_first_letter\n",
    "    print(f\"Downloading data for {ticker_first_letter}...\")\n",
    "    download_frd_data(params=params, zips_dir=ZIPS_DIR, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in tqdm(letters):\n",
    "    extract_frd_zip(\n",
    "        src_zips_dir=ZIPS_DIR, \n",
    "        dest_csv_dir=CSV_DIR,\n",
    "        ticker_first_letter=letter,\n",
    "        period=params['period'],\n",
    "        adjustment=params['adjustment'],\n",
    "        timeframe=params['timeframe'],\n",
    "        overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Minute Stock Bars Adjusted for Splits and Dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"type\": \"stock\",\n",
    "    \"ticker_range\": \"\",\n",
    "    \"timeframe\": \"5min\",\n",
    "    \"adjustment\": \"adj_splitdiv\",\n",
    "    \"period\": \"full\",\n",
    "    'userid': FRD_USER_ID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIPS_DIR, CSV_DIR = make_zips_and_csv_dirs(params)\n",
    "print(f\"ZIPS_DIR: {ZIPS_DIR}\")\n",
    "print(f\"CSV_DIR: {CSV_DIR}\")\n",
    "\n",
    "if not os.path.exists(CSV_DIR):\n",
    "    print(f\"Creating {CSV_DIR}\")\n",
    "    os.makedirs(CSV_DIR)\n",
    "else:\n",
    "    print(f\"{CSV_DIR} exists with {len(os.listdir(CSV_DIR))} files\")\n",
    "if not os.path.exists(ZIPS_DIR):\n",
    "    print(f\"Creating {ZIPS_DIR}\")\n",
    "    os.makedirs(ZIPS_DIR)\n",
    "else:\n",
    "    print(f\"{ZIPS_DIR} exists with {len(os.listdir(ZIPS_DIR))} files\")\n",
    "\n",
    "for ticker_first_letter in letters:\n",
    "    ticker_csv_dir = os.path.join(CSV_DIR, ticker_first_letter)\n",
    "    print(ticker_csv_dir)\n",
    "    \n",
    "    if not os.path.exists(ticker_csv_dir):\n",
    "        os.makedirs(ticker_csv_dir)\n",
    "        print(f\"Creating {ticker_csv_dir}\")\n",
    "    else:\n",
    "        print(f\"{ticker_csv_dir} exists with {len(os.listdir(ticker_csv_dir))} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "#letters = ['X', 'Y']\n",
    "for ticker_first_letter in tqdm(letters):\n",
    "    params['ticker_range'] = ticker_first_letter\n",
    "    print(f\"Downloading data for {ticker_first_letter}...\")\n",
    "    download_frd_data(params=params, zips_dir=ZIPS_DIR, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in tqdm(letters):\n",
    "    extract_frd_zip(\n",
    "        src_zips_dir=ZIPS_DIR, \n",
    "        dest_csv_dir=CSV_DIR,\n",
    "        ticker_first_letter=letter,\n",
    "        period=params['period'],\n",
    "        adjustment=params['adjustment'],\n",
    "        timeframe=params['timeframe'],\n",
    "        overwrite=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Weekly Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"type\": \"stock\",\n",
    "    \"timeframe\": \"1min\",\n",
    "    \"adjustment\": \"adj_splitdiv\",\n",
    "    \"period\": \"week\",\n",
    "    'userid': FRD_USER_ID\n",
    "}\n",
    "\n",
    "ZIPS_DIR, CSV_DIR = make_zips_and_csv_dirs(params)\n",
    "print(f\"ZIPS_DIR: {ZIPS_DIR}\")\n",
    "print(f\"CSV_DIR: {CSV_DIR}\")\n",
    "\n",
    "if not os.path.exists(CSV_DIR):\n",
    "    print(f\"Creating {CSV_DIR}\")\n",
    "    os.makedirs(CSV_DIR)\n",
    "else:\n",
    "    print(f\"{CSV_DIR} exists with {len(os.listdir(CSV_DIR))} files\")\n",
    "if not os.path.exists(ZIPS_DIR):\n",
    "    print(f\"Creating {ZIPS_DIR}\")\n",
    "    os.makedirs(ZIPS_DIR)\n",
    "else:\n",
    "    print(f\"{ZIPS_DIR} exists with {len(os.listdir(ZIPS_DIR))} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_frd_data(params=params, zips_dir=ZIPS_DIR, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frd_zip(\n",
    "    src_zips_dir=ZIPS_DIR, \n",
    "    dest_csv_dir=CSV_DIR,\n",
    "    \n",
    "    period=params['period'],\n",
    "    adjustment=params['adjustment'],\n",
    "    timeframe=params['timeframe'],\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"/home/ubuntu/ibis/data/frd-historical/stock/adj_splitdiv/1min/csv/R/RDFN_full_1min_adjsplitdiv.txt\",\n",
    "                names=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "X['date'] = pd.to_datetime(X['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "X.set_index('date', inplace=True)\n",
    "X.to_csv(os.path.join(DATA_DIR, 'RDFN_full_1min_adjsplitdiv.csv'), index=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv(\"/home/ubuntu/ibis/data/frd-historical/stock_week_20240915/adj_splitdiv/1min/csv/RDFN_week_1min_adjsplitdiv.txt\")\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'RDFN'\n",
    "fp = f\"{ticker}_{period}_{timeframe}_{adjustment.replace('_','')}.txt\"\n",
    "fp = os.path.join(CSV_DIR, fp)\n",
    "print(fp)\n",
    "rdfn_df = pd.read_csv(fp, names=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "rdfn_df['ret'] = rdfn_df['close'].pct_change()\n",
    "rdfn_df['date'] = pd.to_datetime(rdfn_df['date'])\n",
    "rdfn_df.set_index('date', inplace=True)\n",
    "rdfn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add returns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rdfn_df['ret'].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to daily values\n",
    "rdfn_daily_df = rdfn_df.resample('D').agg({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum',\n",
    "})\n",
    "print(rdfn_daily_df.info())\n",
    "rdfn_daily_df['close_to_close_ret'] = rdfn_daily_df['close'].pct_change()\n",
    "rdfn_daily_df['intraday_ret'] = rdfn_daily_df['close']/rdfn_daily_df['open'] - 1\n",
    "rdfn_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(rdfn_df['open'].values, label='Open')\n",
    "plt.plot(rdfn_df['high'].values, label='High')\n",
    "plt.plot(rdfn_df['low'].values, label='Low')\n",
    "plt.plot(rdfn_df['close'].values, label='Close')\n",
    "plt.legend()\n",
    "plt.title(f\"{ticker} {period}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"type\": \"index\",\n",
    "    \"timeframe\": \"1day\",    \n",
    "    \"period\": \"full\",\n",
    "    'userid': FRD_USER_ID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_type = params['type']\n",
    "timeframe = params['timeframe']\n",
    "period = params['period']\n",
    "\n",
    "INDEX_DATA_DIR = os.path.join(\n",
    "    FRD_DATA_DIR, \n",
    "    frd_download_directories.query(f\"type == '{asset_type}' & timeframe == '{timeframe}'\")['directory'].values[0]\n",
    ")\n",
    "print(INDEX_DATA_DIR)\n",
    "if not os.path.exists(INDEX_DATA_DIR):\n",
    "    print(f\"Creating {INDEX_DATA_DIR}\")\n",
    "    os.makedirs(INDEX_DATA_DIR)\n",
    "else:\n",
    "    print(f\"{INDEX_DATA_DIR} exists with {len(os.listdir(INDEX_DATA_DIR))} files\")\n",
    "ZIPS_DIR = os.path.join(INDEX_DATA_DIR, 'zips')\n",
    "CSV_DIR = os.path.join(INDEX_DATA_DIR, 'csv')\n",
    "if not os.path.exists(CSV_DIR):\n",
    "    print(f\"Creating {CSV_DIR}\")\n",
    "    os.makedirs(CSV_DIR)\n",
    "else:\n",
    "    print(f\"{CSV_DIR} exists with {len(os.listdir(CSV_DIR))} files\")\n",
    "if not os.path.exists(ZIPS_DIR):\n",
    "    print(f\"Creating {ZIPS_DIR}\")\n",
    "    os.makedirs(ZIPS_DIR)\n",
    "else:\n",
    "    print(f\"{ZIPS_DIR} exists with {len(os.listdir(ZIPS_DIR))} files\")\n",
    "zip_fp = os.path.join(ZIPS_DIR, f'{asset_type}_{period}_{timeframe}.zip')\n",
    "print(zip_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://firstratedata.com/api/data_file\"\n",
    "response = requests.get(base_url, params=params)\n",
    "print(response.url)\n",
    "if response.status_code == 200:      \n",
    "    with open(zip_fp, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"ZIP file saved: {zip_fp}\")\n",
    "else:\n",
    "    print(f\"Failed to download data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Extracting {zip_fp} to {CSV_DIR}\")\n",
    "with zipfile.ZipFile(zip_fp, 'r') as zip_ref:\n",
    "    zip_ref.extractall(CSV_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tickers based on file names\n",
    "index_tickers = [x.split('_')[0] for x in os.listdir(CSV_DIR) if x.endswith('.txt')]\n",
    "index_tickers[:3], len(index_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first and last date of coverage for each index\n",
    "def index_date_range(ticker):\n",
    "    fp = os.path.join(CSV_DIR, f\"{ticker}_{period}_{timeframe}.txt\")\n",
    "    df = pd.read_csv(fp, names=['date', 'open', 'high', 'low', 'close',])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df['date'].min(), df['date'].max()\n",
    "index_ranges = {ticker: index_date_range(ticker) for ticker in index_tickers}\n",
    "index_ranges_df = pd.DataFrame(index_ranges).T.reset_index()\n",
    "index_ranges_df.columns = ['ticker', 'min_date', 'max_date']\n",
    "index_ranges_df['series_length'] = (index_ranges_df['max_date'] - index_ranges_df['min_date']).dt.days\n",
    "index_ranges_df = index_ranges_df[index_ranges_df['series_length'] > 0]\n",
    "index_ranges_df.sort_values('series_length', ascending=False, inplace=True)\n",
    "index_ranges_df.to_csv(os.path.join(FRD_DATA_DIR, 'index_date_ranges.csv'), index=False)\n",
    "index_ranges_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
